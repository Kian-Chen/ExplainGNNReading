{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stemmer"]},"docs":[{"location":"","title":"Paper Index of XAI for GNNs","text":"<p>Papers about the explainability of GNNs</p>"},{"location":"#surveys","title":"Surveys","text":"<ol> <li>[Proceedings of the IEEE 24] Trustworthy Graph Neural Networks: Aspects, Methods and Trends paper</li> <li>[Preprint 24] Graph-Based Explainable AI: A Comprehensive Survey paper</li> <li>[Arixv 23] A Survey on Explainability of Graph Neural Networks paper</li> <li>[ACM computing survey] A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation, and Research Challenges paper</li> <li>[TPAMI 22]Explainability in graph neural networks: A taxonomic survey. Yuan Hao, Yu Haiyang, Gui Shurui, Ji Shuiwang. paper</li> <li>[Arxiv 22]A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics paper</li> <li>[Arxiv 22] A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection paper</li> <li>[Big Data 2022]A Survey of Explainable Graph Neural Networks for Cyber Malware Analysis paper</li> <li>[Arxiv 23] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainabilitypaper</li> <li>[Arxiv 22] Explaining the Explainers in Graph Neural Networks: a Comparative Study paper</li> <li>[Book 23] Generative Explanation for Graph Neural Network: Methods and Evaluation paper</li> </ol>"},{"location":"#platforms","title":"Platforms","text":"<ol> <li>PyTorch Geometric [Document] [Blog]</li> <li>DIG: A Turnkey Library for Diving into Graph Deep Learning Research paper Code</li> <li>GraphXAI: Evaluating Explainability for Graph Neural Networks paper Code</li> <li>GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks paper Code</li> <li>GNNExplainer and PGExplainer paper Code</li> <li>BAGEL: A Benchmark for Assessing Graph Neural Network Explanations [paper]Code</li> </ol>"},{"location":"#most-influential-papers-selected-by-cogdl","title":"Most Influential Papers selected by Cogdl","text":"<ol> <li>Explainability in graph neural networks: A taxonomic survey. Yuan Hao, Yu Haiyang, Gui Shurui, Ji Shuiwang. ARXIV 2020. paper</li> <li>Gnnexplainer: Generating explanations for graph neural networks. Ying Rex, Bourgeois Dylan, You Jiaxuan, Zitnik Marinka, Leskovec Jure. NeurIPS 2019. paper code</li> <li>Explainability methods for graph convolutional neural networks. Pope Phillip E, Kolouri Soheil, Rostami Mohammad, Martin Charles E, Hoffmann Heiko. CVPR 2019.paper</li> <li>Parameterized Explainer for Graph Neural Network. Luo Dongsheng, Cheng Wei, Xu Dongkuan, Yu Wenchao, Zong Bo, Chen Haifeng, Zhang Xiang. NeurIPS 2020. paper code</li> <li>Xgnn: Towards model-level explanations of graph neural networks. Yuan Hao, Tang Jiliang, Hu Xia, Ji Shuiwang. KDD 2020. paper. </li> <li>Evaluating Attribution for Graph Neural Networks. Sanchez-Lengeling Benjamin, Wei Jennifer, Lee Brian, Reif Emily, Wang Peter, Qian Wesley, McCloskey Kevin, Colwell  Lucy, Wiltschko Alexander. NeurIPS  2020.paper</li> <li>PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks. Vu Minh, Thai My T.. NeurIPS  2020.paper</li> <li>Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks. Federico Baldassarre and Kevin Smith and Josephine Sullivan and Hossein Azizpour. ECCV 2020.paper</li> <li>GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media. Lu, Yi-Ju and Li, Cheng-Te. ACL 2020.paper</li> <li>On Explainability of Graph Neural Networks via Subgraph Explorations. Yuan Hao, Yu Haiyang, Wang Jie, Li Kang, Ji Shuiwang. ICML 2021.paper</li> </ol>"},{"location":"#year-2024","title":"Year 2024","text":"<ol> <li>[NeurIPS 24] RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task [paper]</li> <li>[NeurIPS 24] GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules[paper]</li> <li>[ICML 24] Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks[paper]</li> <li>[ICML 24] Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks [paper]</li> <li>[ICML 24] Graph Neural Network Explanations are Fragile [paper]</li> <li>[ICML 24] How Interpretable Are Interpretable Graph Neural Networks? [paper]</li> <li>[ICML 24] Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation[paper]</li> <li>[ICML 24] Explaining Graph Neural Networks via Structure-aware Interaction Index [paper]</li> <li>[ICML 24] EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time [paper]</li> <li>[ICLR 24] GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks [paper]</li> <li>[ICLR 24] GOAt: Explaining Graph Neural Networks via Graph Output Attribution [paper]</li> <li>[ICLR 24] Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks [paper]</li> <li>[ICLR 24] GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking [paper]</li> <li>[ICLR 24] UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models [paper]</li> <li>[TPAMI 24] Towards Inductive and Efficient Explanations for Graph Neural Networks[paper]</li> <li>[TMLR 24] InduCE: Inductive Counterfactual Explanations for Graph Neural Networks [paper]</li> <li>[PLDI 24] PL4XGL: A Programming Language Approach to Explainable Graph Learning[paper]</li> <li>[Usenix Security 24] INSIGHT: Attacking Industry-Adopted Learning Resilient Logic Locking Techniques Using Explainable Graph Neural Network[paper]</li> <li>[SIGMOD 24]View-based Explanations for Graph Neural Networks [paper]</li> <li>[Thesis UCLA] Explainable Artificial Intelligence for Graph Data[paper]</li> <li>[Thesis UVA] Algorithmic Fairness in Graph Machine Learning: Explanation, Optimization, and Certification[paper]</li> <li>[KDD 24] SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning[paper]</li> <li>[KDD 24] Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck[paper]</li> <li>[KDD 24] Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks[paper]</li> <li>[ICDE 24] Generating Robust Counterfactual Witnesses for Graph Neural Networks [paper]</li> <li>[ICDE 24] SES: Bridging the Gap Between Explainability and Prediction of Graph Neural Networks[paper]</li> <li>[ICSE 24] Coca: Improving and Explaining Graph Neural Network-Based Vulnerability Detection Systems[paper]</li> <li>[AAAI 24] Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks [paper]</li> <li>[AAAI 24] Stratifed GNN Explanations through Sufficient Expansion[paper]</li> <li>[AAAI 24] Factorized Explainer for Graph Neural Networks[paper]</li> <li>[AAAI 24] Self-Interpretable Graph Learning with Sufficient and Necessary Explanations</li> <li>[AAAI 24] Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder [paper]</li> <li>[AISTATS 24] Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process [paper]</li> <li>[WWW 24] Game-theoretic Counterfactual Explanation for Graph Neural Networks [paper]</li> <li>[WWW 24] EXGC: Bridging Efficiency and Explainability in Graph Condensation[paper]</li> <li>[WWW 24] Adversarial Mask Explainer for Graph Neural Networks [paper]</li> <li>[WWW 24] Globally Interpretable Graph Learning via Distribution Matching[paper]</li> <li>[WWW 24] GNNShap: Scalable and Accurate GNN Explanation using Shapley Values [paper]</li> <li>[TAI 24] Learning Counterfactual Explanation of Graph Neural Networks via Generative Flow Network[paper]</li> <li>[IEEE TMI 24] Multi-Modal Diagnosis of Alzheimer\u2019s Disease using Interpretable Graph Convolutional Networks[paper]</li> <li>[IEEE IoT 24] EXVul: Toward Effective and Explainable Vulnerability Detection for IoT Devices[paper]</li> <li>[ECML/PKDD 24] Towards Few-shot Self-explaining Graph Neural Networks[paper]</li> <li>[SDM 24] XGExplainer: Robust Evaluation-based Explanation for Graph Neural Networks[paper]</li> <li>[DASFAA 24] Multi-objective Graph Neural Network Explanatory Model with Local and Global Information Preservation[paper]</li> <li>[ISSTA 2024] Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation [paper]</li> <li>[KBS 24] Shapley-based graph explanation in embedding space[paper]</li> <li>[KBS 24] GEAR: Learning graph neural network explainer via adjusting gradients[paper]</li> <li>[IEEE TNSM 24] Ensemble Graph Attention Networks for Cellular Network Analytics: From Model Creation to Explainability[paper]</li> <li>[IEEE TNSE 24] GAXG: A Global and Self-adaptive Optimal Graph Topology Generation Framework for Explaining Graph Neural Networks[paper]</li> <li>[IEEE TETCI 24] GF-LRP: A Method for Explaining Predictions Made by Variational Graph Auto-Encoders[paper]</li> <li>[AAAI workshop] Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease[paper]</li> <li>[xAI 24] Global Concept Explanations for Graphs by Contrastive Learning [paper]</li> <li>[Arxiv 24.09] PAGE: Parametric Generative Explainer for Graph Neural Network [paper]</li> <li>[Arxiv 24.09] Higher Order Structures For Graph Explanations [paper]</li> <li>[Arxiv 24.08] SE-SGformer: A Self-Explainable Signed Graph Transformer for Link Sign Prediction[paper]</li> <li>[Preprint 24.08] CIDER: Counterfactual-Invariant Diffusion-based GNN Explainer for Causal Subgraph Inference[paper]</li> <li>[Arxiv 24.07] LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation[paper]</li> <li>[Arxiv 24.07] xAI-Drop: Don't Use What You Cannot Explain[paper]</li> <li>[Arxiv 24.07] Explaining Graph Neural Networks for Node Similarity on Graphs[paper]</li> <li>[Arxiv 24.07] SLInterpreter: An Exploratory and Iterative Human-AI Collaborative System for GNN-based Synthetic Lethal Prediction[paper]</li> <li>[Arxiv 24.07] Graph Neural Network Causal Explanation via Neural Causal Models[paper]</li> <li>[Arxiv 24.06] GNNAnatomy: Systematic Generation and Evaluation of Multi-Level Explanations for Graph Neural Networks[paper]</li> <li>[Arxiv 24.06] On GNN explanability with activation rules[paper]</li> <li>[Arxiv 24.06] Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks[paper]</li> <li>[Arxiv 24.05] SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs[paper]</li> <li>[Arxiv 24.06] Towards Understanding Sensitive and Decisive Patterns in Explainable AI: A Case Study of Model Interpretation in Geometric Deep Learning[paper]</li> <li>[Arxiv 24.06] Explainable Graph Neural Networks Under Fire [paper]</li> <li>[Arxiv 24.06] Explainable AI Security: Exploring Robustness of Graph Neural Networks to Adversarial Attacks [paper]</li> <li>[Arxiv 24.06] Perks and Pitfalls of Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs [paper]</li> <li>[Arxiv 24.05] Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks [paper]</li> <li>[Arxiv 24.05] MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation [paper]</li> <li>[Arxiv 24.05] Detecting Complex Multi-step Attacks with Explainable Graph Neural Network [paper]</li> <li>[Arxiv 24.05] SynHING: Synthetic Heterogeneous Information Network Generation for Graph Learning and Explanation[paper]</li> <li>[Preprint 24.05] Explainable Graph Neural Networks: An Application to Open Statistics Knowledge Graphs for Estimating House Prices [paper]</li> <li>[Arxiv 24.04] Superior Polymeric Gas Separation Membrane Designed by Explainable Graph Machine Learning [paper]</li> <li>[Arxiv 24.04] Improving the interpretability of GNN predictions through conformal-based graph sparsification [paper]</li> <li>[Arxiv 24.03] GreeDy and CoDy: Counterfactual Explainers for Dynamic Graph[paper]</li> <li>[Arxiv 24.03] Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation[paper]</li> <li>[Arxiv 24.03] Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs[paper]</li> <li>[Arixv 24.03] Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations[paper]</li> <li>[Arxiv 24.03] Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation[paper]</li> <li>[Arxiv 24.02] PAC Learnability under Explanation-Preserving Graph Perturbations[paper]</li> <li>[Arxiv 24.02] Explainable Global Wildfire Prediction Models using Graph Neural Networks[paper]</li> <li>[Arxiv 24.02] Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks[paper]</li> <li>[Arxiv 24.01] On Discprecncies between Perturbation Evaluations of Graph Neural Network Attributions[paper]</li> <li>[ASP=DAC 24] LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking[paper]</li> <li>[Biorxiv 24] Community-aware explanations in knowledge graphs with XP-GNN[paper]</li> <li>[ISCV 24] Adaptive Subgraph Feature Extraction for Explainable Multi-Modal Learning[paper]</li> <li>[IJCNN] Explanations for Graph Neural Networks using A Game-theoretic Value[paper]</li> <li>[Neurocomputing] GeoExplainer: Interpreting Graph Convolutional Networks with geometric masking[paper]</li> <li>[Technologies] Explainable Graph Neural Networks: An Application to Open Statistics Knowledge Graphs for Estimating House Prices[paper]</li> <li>[Reliab. Eng. Syst. Saf.] Causal intervention graph neural network for fault diagnosis of complex industrial processes[paper]</li> <li>[Frontiers in big data] Global explanation supervision for Graph Neural Networks[paper]</li> <li>[Information and Software Technology] Graph-based explainable vulnerability prediction[paper]</li> <li>[Information Systems] Heterogeneous graph neural networks for fraud detection and explanation in supply chain finance[paper]</li> <li>[Information Procs. &amp; Mana.] Towards explaining graph neural networks via preserving prediction ranking and structural dependency[paper]</li> <li>[Applied Energy]  Explainable Spatio-Temporal Graph Neural Networks for multi-site photovoltaic energy production [paper]</li> <li>[PAKDD 24] Random Mask Perturbation Based Explainable Method of Graph Neural Networks [paper]</li> <li>[Computational Materials Science] Graph isomorphism network for materials property prediction along with explainability analysis[paper]</li> <li>[NN 24] Explanatory subgraph attacks against Graph Neural Networks[paper]</li> <li>[NN 24] GRAM: An interpretable approach for graph anomaly detection using gradient attention maps[paper]</li> <li>[Neural Networks 24] CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis [paper]</li> <li>[NeuroImage 24] BPI-GNN: Interpretable brain network-based psychiatric diagnosis and subtyping[paper]</li> <li>[PAKDD 24] Toward Interpretable Graph Classification via Concept-Focused Structural Correspondence [paper]</li> <li>[MedRxiv 24] An Interpretable Population Graph Network to Identify Rapid Progression of Alzheimer\u2019s Disease Using UK Biobank[paper]</li> <li>[IEEE TDSC 24] TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support [paper]</li> <li>[IEEE Transactions] IEEE Transactions on Computational Social Systems[paper]</li> <li>[Journal of Physics] Explainer on GNN-based segmentation networks[paper]</li> <li>[Energy and AI] Electricity demand forecasting at distribution and household levels using explainable causal graph neural network [paper]</li> </ol>"},{"location":"#year-2023","title":"Year 2023","text":"<ol> <li>[NeurIPS 23] Interpretable Graph Networks Formulate Universal Algebra Conjectures[paper]</li> <li>[NeurIPS 23] SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanation [paper]</li> <li>[NeurIPS 23] Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks[paper]</li> <li>[NeurIPS 23] D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion [paper]</li> <li>[NeurIPS 23] TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery [paper]</li> <li>[NeurIPS 23] V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs [paper]</li> <li>[NeurIPS 23] Towards Self-Interpretable Graph-Level Anomaly Detection [paper]</li> <li>[NeurIPS 23] Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis [paper]</li> <li>[NeurIPS 23] Interpretable Prototype-based Graph Information Bottleneck [paper]</li> <li>[ICML 23] Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching [paper]</li> <li>[ICML 23] Relevant Walk Search for Explaining Graph Neural Networks [paper]</li> <li>[ICML 23] Towards Understanding the Generalization of Graph Neural Networks [paper]</li> <li>[ICLR 23] GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks [paper]</li> <li>[ICLR 23] Global Explainability of GNNs via Logic Combination of Learned Concepts [paper]</li> <li>[ICLR 23] Explaining Temporal Graph Models through an Explorer-Navigator Framework [paper]</li> <li>[ICLR 23] DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks [paper]</li> <li>[ICLR 23] Interpretable Geometric Deep Learning via Learnable Randomness Injection [paper]</li> <li>[ICLR 23] A Differential Geometric View and Explainability of GNN on Evolving Graphs [paper]</li> <li>[KDD 23] MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation [paper]</li> <li>[KDD 23] Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation [paper]</li> <li>[KDD 23] Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective[paper]</li> <li>[KDD 23] Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining.[paper]</li> <li>[KDD 23] Shift-Robust Molecular Relational Learning with Causal Substructure [paper]</li> <li>[AAAI 23] Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis [paper]</li> <li>[AAAI 23] On the Limit of Explaining Black-box Temporal Graph Neural Networks [paper]</li> <li>[AAAI 23] Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network [paper]</li> <li>[AAAI 23] Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery [paper]</li> <li>[VLDB 23] HENCE-X: Toward Heterogeneity-agnostic Multi-level Explainability for Deep Graph Networks [paper]</li> <li>[VLDB 23] On Data-Aware Global Explainability of Graph Neural Networks [paper]</li> <li>[AISTATS 23] Distill n' Explain: explaining graph neural networks using simple surrogates [Paper]</li> <li>[AISTATS 23] Probing Graph Representations [paper]</li> <li>[ICDE 23] INGREX: An Interactive Explanation Framework for Graph Neural Networks[paper]</li> <li>[ICDE 23] Jointly Attacking Graph Neural Network and its Explanations [paper]</li> <li>[WWW 23]PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction [paper]</li> <li>[ICDM 23] Limitations of Perturbation-based Explanation Methods for Temporal Graph Neural Networks</li> <li>[ICDM 23] Interpretable Subgraph Feature Extraction for Hyperlink Prediction[paper]</li> <li>[WSDM 23]Interpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs [paper]</li> <li>[WSDM 23]Cooperative Explanations of Graph Neural Networks [paper]</li> <li>[WSDM 23]Towards Faithful and Consistent Explanations for Graph Neural Networks [paper]</li> <li>[WSDM 23] Global Counterfactual Explainer for Graph Neural Networks [paper]</li> <li>[CIKM 23] Explainable Spatio-Temporal Graph Neural Networks [paper]</li> <li>[CIKM 23] DuoGAT: Dual Time-oriented Graph Attention Networks for Accurate, Efficient and Explainable Anomaly Detection on Time-series. [paper]</li> <li>[CIKM 23] Heterogeneous Temporal Graph Neural Network Explainer [paper]</li> <li>[CIKM 23] ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks[paper]</li> <li>[CIKM 23] KG4Ex: An Explainable Knowledge Graph-Based Approach for Exercise Recommendation [paper]</li> <li>[ECML-PKDD 23] ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning [paper]</li> <li>[TPAMI 23] FlowX: Towards Explainable Graph Neural Networks via Message Flows [paper]</li> <li>[TAI] Prototype-based interpretable graph neural networks. [paper]</li> <li>[TKDE 23] Counterfactual Graph Learning for Anomaly Detection on Attributed Networks [paper]</li> <li>[Scientific Data 23 ] Evaluating explainability for graph neural networks [paper]</li> <li>[Nature Communications 23] Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking [paper]</li> <li>[ACM Computing Surveys 23] A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation [paper]</li> <li>[TIST 23] Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment [paper]</li> <li>[Openreview 23] STExplainer: Global Explainability of GNNs via Frequent SubTree Mining [paper]</li> <li>[GLFrontiers 23] Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts [paper]</li> <li>[Openreview 23] Iterative Graph Neural Network Enhancement Using Explanations [paper]</li> <li>[Openreview 23] Interpretable and Convergent Graph Neural Network Layers at Scale [paper]</li> <li>[NeurIPS 2023 Workshop XAIA] GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Networks Explanations [paper]</li> <li>[NeurIPS 2023 Workshop XAIA] On the Consistency of GNN Explainability Methods [paper]</li> <li>[Arxiv 23] Evaluating Neighbor Explainability for Graph Neural Networks [paper]</li> <li>[Arxiv 23] DyExplainer: Explainable Dynamic Graph Neural Networks [paper]</li> <li>[Arxiv 23] Explainability-Based Adversarial Attack on Graphs Through Edge Perturbation[paper]</li> <li>[AICS 23] A subgraph interpretation generative model for knowledge graph link prediction based on uni-relation transformation [paper]</li> <li>[GUT 23] Screening of normal endoscopic large bowel biopsies with interpretable graph learning: a retrospective study [paper]</li> <li>[PR 2023] Towards self-explainable graph convolutional neural network with frequency adaptive inception [paper]</li> <li>[MLG 2023] Understanding how explainers work in graph neural networks [paper]</li> <li>[MLG 2023] Graph Model Explainer Tool [paper]</li> <li>[Information Science 23] Robust explanations for graph neural network with neuron explanation component [paper]</li> <li>[Recsys 23] Explainable Graph Neural Network Recommenders; Challenges and Opportunities [paper]</li> <li>[xAI  23] Counterfactual Explanations for Graph Classification Through the Lenses of Density [paper]</li> <li>[XAI 23] Evaluating Link Prediction Explanations for Graph Neural Networks [[paper]](https://arxiv.org/abs/2308.01682</li> <li>[xAI 23] XInsight: Revealing Model Insights for GNNs with Flow-based Explanations [paper]</li> <li>[xAI 23] Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies [paper]</li> <li>[xAI 23] MEGAN: Multi Explanation Graph Attention Network [paper]</li> <li>[XKDD 23] Game Theoretic Explanations for Graph Neural Networks [paper]</li> <li>[XKDD 23] From Black Box to Glass Box: Evaluating Faithfulness of Process Predictions with GCNNs [paper]</li> <li>[IJCNN 23] MEGA: Explaining Graph Neural Networks with Network Motifs [paper]</li> <li>[LOG Poster 23] On the Robustness of Post-hoc GNN Explainers to Label Noise [paper]</li> <li>[LOG Poster 23] How Faithful are Self-Explainable GNNs? [paper]</li> <li>[LOG Poster 23] Explaining Link Predictions in Knowledge Graph Embedding Models with Influential Examples [paper]</li> <li>[Bioriv 23] Building explainable graph neural network by sparse learning for the drug-protein binding prediction [paper]</li> <li>[ICAID 2023] Explanations for Graph Neural Networks via Layer Analysis. [paper]</li> <li>[ECAI 23] XGBD: Explanation-Guided Graph Backdoor Detection [paper]</li> <li>[IEEE Transactions on Consumer Electronics 23] Human Pose Prediction Using Interpretable Graph Convolutional Network for Smart Home [paper]</li> <li>[KBS 23] KE-X: Towards subgraph explanations of knowledge graph embedding based on knowledge information gain [paper]</li> <li>[ICML workshop 23] Generating Global Factual and Counterfactual Explainer for Molecule under Domain Constraints [paper]</li> <li>[Thesis 23] Developing interpretable graph neural networks for high dimensional feature spaces [paper]</li> <li>[Thesis 23] Evaluation of Explainability Methods on Single-Cell Classification Tasks Using Graph Neural Networks [paper]</li> <li>[Arxiv 23] On the Interplay of Subset Selection and Informed Graph Neural Networks [paper]</li> <li>[ISSTA23] Interpreters for GNN-Based Vulnerability Detection: Are We There Yet? [paper]</li> <li>[ICECAI23] Improved GraphSVX for GNN Explanations Based on Cross Entropy [paper]</li> <li>[ICRA Workshop 23] Towards Semantic Interpretation and Validation of Graph Attention-based Explanations [paper]</li> <li>[Arxiv 23] Graph Neural Network based Log Anomaly Detection and Explanation [paper]</li> <li>[Arxiv 23] Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features [paper]</li> <li>[Thesis 23] Interpretability of Graphical Models [paper]</li> <li>[Bioengineering 2023] Personalized Explanations for Early Diagnosis of Alzheimer's Disease Using Explainable Graph Neural Networks with Population Graphs [paper]</li> <li>[BDSC 2023] MDC: An Interpretable GNNs Method Based on Node Motif Degree and Graph Diffusion Convolution [[paper]] (https://link.springer.com/chapter/10.1007/978-981-99-3925-1_24)</li> <li>[Information Science 2023] Explainability techniques applied to road traffic forecasting using Graph Neural Network models [paper]</li> <li>[Arxiv 23] Efficient GNN Explanation via Learning Removal-based Attribution [paper]</li> <li>[Arxiv 23] Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity [paper]</li> <li>[ICLR Tiny 23] Message-passing selection: Towards interpretable GNNs for graph classification [paper]</li> <li>[ICLR Tiny 23] Revisiting CounteRGAN for Counterfactual Explainability of Graphs [paper]</li> <li>[MICCAI Workshop 23] IA-GCN: Interpretable Attention based Graph Convolutional Network for Disease prediction [paper]</li> <li>[Arxiv 23] Robust Ante-hoc Graph Explainer using Bilevel Optimization [paper]</li> <li>[GRADES &amp; NDA'23] A Demonstration of Interpretability Methods for Graph Neural Networks [paper]</li> <li>[Arxiv 23] Self-Explainable Graph Neural Networks for Link Prediction [paper]</li> <li>[ChemRxiv 23] Interpreting Graph Neural Networks with Myerson Values for Cheminformatics Approaches [paper]</li> <li>[Neural Networks 23] Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness [paper]</li> <li>[ICASSP 23] Towards a More Stable and General Subgraph Information Bottleneck [paper]</li> <li>[ESANN 23] Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability [Paper]</li> <li>[IEEE Access] Generating Real-Time Explanations for GNNs via Multiple Specialty Learners and Online Knowledge Distillation [Paper]</li> <li>[IEEE Access] Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions [paper]</li> <li>[Journal of Software 23] A Slice-level vulnerability detection and interpretation method based on graph neural network [paper]</li> <li>[Automation in Construction 23] Learning from explainable data-driven tunneling graphs: A spatio-temporal graph convolutional network for clogging detection [paper]</li> <li>[Briefings in Bioinformatics] Predicting molecular properties based on the interpretable graph neural network with multistep focus mechanism [paper]</li> <li>[Briefings in Bioinformatics] Identification of vital chemical information via visualization of graph neural networks [paper]</li> <li>[Bioinformatics 23] Explainable Multilayer Graph Neural Network for Cancer Gene Prediction [paper]</li> <li>[ICLR Workshop 23] GCI: A Graph Concept Interpretation Framework [paper]</li> <li>[Arxiv 23] Structural Explanations for Graph Neural Networks using HSIC [paper]</li> <li>[Internet of Things 23] XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics [paper]</li> <li>[JOS23] A Generic Explaining &amp; Locating Method for Malware Detection based on Graph Neural Networks [paper]</li> </ol>"},{"location":"#year-2022","title":"Year 2022","text":"<ol> <li>[NeurIPS 22] GStarX:Explaining Graph-level Predictions with Communication Structure-Aware Cooperative Games [paper]</li> <li>[NeurIPS 22] Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure [paper]</li> <li>[NeurIPS 22] Task-Agnostic Graph Neural Explanations [paper]</li> <li>[NeurIPS 22] CLEAR: Generative Counterfactual Explanations on Graphs[paper]</li> <li>[ICML 22] Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism [paper]</li> <li>[ICLR 22] DEGREE: Decomposition Based Explanation for Graph Neural Networks [paper]</li> <li>[ICLR 22] Explainable GNN-Based Models over Knowledge Graphs [paper]</li> <li>[ICLR 22] Discovering Invariant Rationales for Graph Neural Networks [paper]</li> <li>[KDD 22] On Structural Explanation of Bias in Graph Neural Networks [paper]</li> <li>[KDD 22] Causal Attention for Interpretable and Generalizable Graph Classification [paper]</li> <li>[CVPR 22] OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks [paper]</li> <li>[CVPR 22] Improving Subgraph Recognition with Variational Graph Information Bottleneck [paper]</li> <li>[AISTATS 22] Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods [paper]</li> <li>[AISTATS 22] CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks [paper]</li> <li>[TPAMI 22] Differentially Private Graph Neural Networks for Whole-Graph Classification [paper]</li> <li>[TPAMI 22] Reinforced Causal Explainer for Graph Neural Networks [paper]</li> <li>[VLDB 22] xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs [paper]</li> <li>[LOG 22]GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks [paper]</li> <li>[LOG 22] Towards Training GNNs using Explanation Directed Message Passing [paper]</li> <li>[The Webconf 22] Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning [paper]</li> <li>[AAAI 22] Prototype-Based Explanations for Graph Neural Networks [paper]</li> <li>[AAAI 22] KerGNNs: Interpretable Graph Neural Networks with Graph Kernels[paper]</li> <li>[AAAI 22] ProtGNN: Towards Self-Explaining Graph Neural Networks [paper]</li> <li>[IEEE Big Data 22] Trade less Accuracy for Fairness and Trade-off Explanation for GNN [paper]</li> <li>[CIKM 22] GRETEL: A unified framework for Graph Counterfactual Explanation Evaluation [paper]</li> <li>[CIKM 22] GRETEL: Graph Counterfactual Explanation Evaluation Framework[paper]</li> <li>[CIKM 22] A Model-Centric Explainer for Graph Neural Network based Node Classification [paper]</li> <li>[IJCAI 22] What Does My GNN Really Capture? On Exploring Internal GNN Representations [paper]</li> <li>[ECML PKDD 22] Improving the quality of rule-based GNN explanations [paper]</li> <li>[MICCAI 22] Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis [paper]</li> <li>[MICCAI 22] Sparse Interpretation of Graph Convolutional Networks for Multi-modal Diagnosis of Alzheimer\u2019s Disease [paper]</li> <li>[EuroS&amp;P 22] Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis [paper]</li> <li>[INFOCOM 22] Interpretability Evaluation of Botnet Detection Model based on Graph Neural Network [paper]</li> <li>[GLOBECOM 22] Shapley Explainer - An Interpretation Method for GNNs Used in SDN [paper]</li> <li>[GLOBECOM 22] An Explainer for Temporal Graph Neural Networks [paper]</li> <li>[TKDE 22] Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks [paper]</li> <li>[TNNLS 22] Interpretable Graph Reservoir Computing With the Temporal Pattern Attention [paper]</li> <li>[TNNLS22] A Meta-Learning Approach for Training Explainable Graph Neural Networks [paper]</li> <li>[TNNLS 22] Explaining Deep Graph Networks via Input Perturbation [paper]</li> <li>[TNNLS 22] A Meta-Learning Approach for Training Explainable Graph Neural Network [paper]</li> <li>[DMKD 22] On GNN explanability with activation patterns [paper]</li> <li>[KBS 22] EGNN: Constructing explainable graph neural networks via knowledge distillation [paper]</li> <li>[XKDD 22] GREASE: Generate Factual and Counterfactual Explanations for GNN-based Recommendations [paper]</li> <li>[AI 22] Are Graph Neural Network Explainers Robust to Graph Noises? [paper]</li> <li>[BRACIS 22] ConveXplainer for Graph Neural Networks [paper]</li> <li>[GLB 22] An Explainable AI Library for Benchmarking Graph Explainers [paper]</li> <li>[DASFAA 22] On Global Explainability of Graph Neural Networks [paper]</li> <li>[ISBI 22] Interpretable Graph Convolutional Network Of Multi-Modality Brain Imaging For Alzheimer\u2019s Disease Diagnosis [paper]</li> <li>[Bioinformatics] GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks [paper]</li> <li>[Medical Imaging 2022]  Phenotype guided interpretable graph convolutional network analysis of fMRI data reveals changing brain connectivity during adolescence [paper]</li> <li>[NeuroComputing 22] Perturb more, trap more: Understanding behaviors of graph neural networks [paper]</li> <li>[DSN 22] CFGExplainer: Explaining Graph Neural Network-Based Malware Classification from Control Flow Graphs [paper]</li> <li>[IEEE Access 22] Providing Node-level Local Explanation for node2vec through Reinforcement Learning [paper]</li> <li>[Patterns 22] Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction [paper]</li> <li>[Arxiv 22] GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers [paper]</li> <li>[IEEE Access 22] Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions [paper]</li> <li>[IEEE 22] Explaining Graph Neural Networks With Topology-Aware Node Selection: Application in Air Quality Inference [paper]</li> <li>[BioRxiv 22] GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks [paper]</li> <li>[IEEE Robotics and Automation Letters 22] Efficient and Interpretable Robot Manipulation with Graph Neural Networks [paper]</li> <li>[Arxiv 22] Deconfounding to Explanation Evaluation in Graph Neural Networks [paper]</li> <li>[ICCPR 22] GANExplainer: GAN-based Graph Neural Networks Explainer [paper]</li> <li>[Arxiv 22] On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach [paper]</li> <li>[Arxiv 22] Exploring Explainability Methods for Graph Neural Networks [paper]</li> <li>[Arxiv 22] PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks [paper]</li> <li>[Arxiv 22] Toward Multiple Specialty Learners for Explaining GNNs via Online Knowledge Distillation [paper]</li> <li>[Openreview 23] TGP: Explainable Temporal Graph Neural Networks for Personalized Recommendation [paper]</li> <li>[Openreview 23] On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective [paper]</li> <li>[Arxiv 22] L2XGNN: Learning to Explain Graph Neural Networks [paper]</li> <li>[Arxiv 22] Towards Prototype-Based Self-Explainable Graph Neural Network [paper]</li> <li>[Arxiv 22] PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge Distillation Processes [paper]</li> <li>[Arxiv 22] Explainability in subgraphs-enhanced Graph Neural Networks [paper]</li> <li>[Arxiv 22] Defending Against Backdoor Attack on Graph Neural Network by Explainability [paper]</li> <li>[Arxiv 22] Explaining Dynamic Graph Neural Networks via Relevance Back-propagation [paper]</li> <li>[Arxiv 22] EiX-GNN : Concept-level eigencentrality explainer for graph neural networks [paper]</li> <li>[Arxiv 22] MotifExplainer: a Motif-based Graph Neural Network Explainer [paper]</li> <li>[Arxiv 22] Faithful Explanations for Deep Graph Models [paper]</li> <li>[Arxiv 22] Towards Explanation for Unsupervised Graph-Level Representation Learning [paper]</li> <li>[Arxiv 22] BAGEL: A Benchmark for Assessing Graph Neural Network Explanations [paper]</li> <li>[Arxiv 22] BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck [paper]</li> <li>[Arxiv 22] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability [paper]</li> <li>[Arxiv 22] Explainability in Graph Neural Networks: An Experimental Survey [paper]</li> <li>[IEEE TSIPN 22] Explainability and Graph Learning from Social Interactions [paper]</li> <li>[Arxiv 22] Cognitive Explainers of Graph Neural Networks Based on Medical Concepts [paper]</li> </ol>"},{"location":"#year-2021","title":"Year 2021","text":"<ol> <li>[NeurIPS 21] SALKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning [paper]</li> <li>[NeurIPS 2021] Reinforcement Learning Enhanced Explainer for Graph Neural Networks [paper]</li> <li>[NeurIPS 2021] Towards Multi-Grained Explainability for Graph Neural Networks [paper]</li> <li>[NeurIPS 2021] Robust Counterfactual Explanations on Graph Neural Networks [paper]</li> <li>[ICML 2021] On Explainability of Graph Neural Networks via Subgraph Explorations[paper]</li> <li>[ICML 2021] Generative Causal Explanations for Graph Neural Networks[paper]</li> <li>[ICML 2021] Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity[paper]</li> <li>[ICML 2021] Automated Graph Representation Learning with Hyperparameter Importance Explanation[paper]</li> <li>[ICLR 2021] Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking[paper]</li> <li>[ICLR 2021] Graph Information Bottleneck for Subgraph Recognition [paper]</li> <li>[KDD 2021] When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods[paper]</li> <li>[KDD 2021] Counterfactual Graphs for Explainable Classification of Brain Networks [paper]</li> <li>[CVPR 2021] Quantifying Explainers of Graph Neural Networks in Computational Pathology.[paper]</li> <li>[NAACL 2021] Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network. [paper]</li> <li>[AAAI 2021] Motif-Driven Contrastive Learning of Graph Representations [paper]</li> <li>[TPAMI 21] Higher-Order Explanations of Graph Neural Networks via Relevant Walks [paper]</li> <li>[WWW 2021] Interpreting and Unifying Graph Neural Networks with An Optimization Framework [paper]</li> <li>[Genome medicine 21] Explaining decisions of Graph Convolutional Neural Networks: patient-specific molecular subnetworks responsible for metastasis prediction in breast cancer [paper]</li> <li>[IJCKG 21] Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules [paper]</li> <li>[RuleML+RR 21] Combining Sub-Symbolic and Symbolic Methods for Explainability [paper]</li> <li>[PAKDD 21] SCARLET: Explainable Attention based Graph Neural Network for Fake News spreader prediction [paper]</li> <li>[J. Chem. Inf. Model] Coloring Molecules with Explainable Artificial Intelligence for Preclinical Relevance Assessment [paper]</li> <li>[BioRxiv 21] APRILE: Exploring the Molecular Mechanisms of Drug Side Effects with Explainable Graph Neural Networks [paper]</li> <li>[ISM 21] Edge-Level Explanations for Graph Neural Networks by Extending Explainability Methods for Convolutional Neural Networks [paper]</li> <li>[Arxiv 21] Towards the Explanation of Graph Neural Networks in Digital Pathology with Information Flows [paper]</li> <li>[Arxiv 21] SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods [paper]</li> <li>[Arxiv 21] Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation [paper]</li> <li>[Arxiv 21] Learnt Sparsification for Interpretable Graph Neural Networks [paper]</li> <li>[ICML workshop 21] GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks [paper]</li> <li>[ICML workshop 21] Reliable Graph Neural Network Explanations Through Adversarial Training [paper]</li> <li>[ICML workshop 21] Reimagining GNN Explanations with ideas from Tabular Data [paper]</li> <li>[ICML workshop 21] Towards Automated Evaluation of Explanations in Graph Neural Networks [paper]</li> <li>[ICDM 2021] GNES: Learning to Explain Graph Neural Networks [paper]</li> <li>[ICDM 2021] GCN-SE: Attention as Explainability for Node Classification in Dynamic Graphs [paper]</li> <li>[ICDM 2021] Multi-objective Explanations of GNN Predictions [paper]</li> <li>[CIKM 2021] Towards Self-Explainable Graph Neural Network [paper]</li> <li>[ECML PKDD 2021] GraphSVX: Shapley Value Explanations for Graph Neural Networks [paper]</li> <li>[WiseML 2021] Explainability-based Backdoor Attacks Against Graph Neural Networks [paper]</li> <li>[IJCNN 21] MEG: Generating Molecular Counterfactual Explanations for Deep Graph Networks [paper]</li> <li>[ICCSA 2021] Understanding Drug Abuse Social Network Using Weighted Graph Neural Networks Explainer [paper]</li> <li>[NeSy 21] A New Concept for Explaining Graph Neural Networks [paper]</li> <li>[Information Fusion 21] Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI [paper]</li> <li>[Patterns 21] hcga: Highly Comparative Graph Analysis for network phenotyping [paper]</li> </ol>"},{"location":"#year-2020-and-before","title":"Year 2020 and Before","text":"<ol> <li>[NeurIPS 2020] Parameterized Explainer for Graph Neural Network.[paper]</li> <li>[NeurIPS 2020] PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks [paper]</li> <li>[KDD 2020] XGNN: Towards Model-Level Explanations of Graph Neural Networks [paper]</li> <li>[ACL 2020]GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media. paper</li> <li>[Arxiv 2020] Graph Neural Networks Including Sparse Interpretability [paper]</li> <li>[NeurIPS Workshop 20] Towards explainable message passing networks for predicting carbon dioxide adsorption in metal-organic frameworks [paper]</li> <li>[ICML workstop 2020] Contrastive Graph Neural Network Explanation [paper]</li> <li>[ICML workstop 2020] Towards Explainable Graph Representations in Digital Pathology [paper]</li> <li>[NeurIPS workshop 2020] Explaining Deep Graph Networks with Molecular Counterfactuals [paper]</li> <li>[DataMod 2020] Exploring Graph-Based Neural Networks for Automatic Brain Tumor Segmentation\" [paper]</li> <li>[OpenReview 20] A Framework For Differentiable Discovery Of Graph Algorithms [paper]</li> <li>[OpenReview 20] Causal Screening to Interpret Graph Neural Networks [paper]</li> <li>[Arxiv 20] Understanding Graph Neural Networks from Graph Signal Denoising Perspectives [paper]</li> <li>[Arxiv 20] Understanding the Message Passing in Graph Neural Networks via Power Iteration [paper]</li> <li>[Arxiv 20] xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links [paper]</li> <li>[IJCNN 20] GCN-LRP explanation: exploring latent attention of graph convolutional networks] [paper]</li> <li>[CD-MAKE 20] Explain Graph Neural Networks to Understand Weighted Graph Features in Node Classification [paper] </li> <li>[ICDM 19] Scalable Explanation of Inferences on Large Graphs[paper] </li> </ol>"},{"location":"surveys/hal-04660442/hal-04660442/","title":"Graph-Based Explainable AI: A Comprehensive Survey","text":"<p>code</p>"},{"location":"surveys/hal-04660442/hal-04660442/#0-overview","title":"0. Overview","text":"<pre><code>graph TD\n    style A fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style B fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style C fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style D fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n\n    style E fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style F fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style G fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n    style H fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n    style I fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n\n    style J fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style K fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style L fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n\n    A[Explainability on Graphs]\n    B[Scoring] --&gt; E[Gradients]\n    B[Scoring] --&gt; F[Decomposition]\n\n    A --&gt; B[Scoring]\n    A --&gt; C[Extraction]\n    A --&gt; D[Generation]\n\n    C[Extraction] --&gt; G[Sequential Paths]\n    C[Extraction] --&gt; H[Logic Rules]\n    C[Extraction] --&gt; I[Subgraph]\n\n    G[Sequential Paths] --&gt; J[Path Reasoning]\n    H[Logic Rules] --&gt; K[Data Integration]\n    G --&gt; K\n    H[Logic Rules] --&gt; L[Surrogate]\n    I[Subgraph] --&gt;J\n    I[Subgraph] --&gt;K\n    I[Subgraph] --&gt; M[Perturbation]\n    I[Subgraph] --&gt; N[Graph Creation]\n\n    D --&gt; N\n\n    style M fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style N fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px</code></pre>"},{"location":"topconf/AAAI/","title":"Paper Index of XAI for GNNs in AAAI","text":""},{"location":"topconf/AAAI/#year-2024","title":"Year 2024","text":"<p>1. Factorized Explainer for Graph Neural Networks.(1) Rundong Huang, Farhad Shirani, Dongsheng Luo. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed factorized explainer.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2023","title":"Year 2023","text":"<p>1. Interpreting Unfairness in Graph Neural Networks via Training Node Attribution.(1) Yushun Dong, Song Wang, Jing Ma, Ninghao Liu, Jundong Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have emerged as the leading paradigm for solving graph analytical problems in various real-world applications. Nevertheless, GNNs could potentially render biased predictions towards certain demographic subgroups. Understanding how the bias in predictions arises is critical, as it guides the design of GNN debiasing mechanisms. However, most existing works overwhelmingly focus on GNN debiasing, but fall short on explaining how such bias is induced. In this paper, we study a novel problem of interpreting GNN unfairness through attributing it to the influence of training nodes. Specifically, we propose a novel strategy named Probabilistic Distribution Disparity (PDD) to measure the bias exhibited in GNNs, and develop an algorithm to efficiently estimate the influence of each training node on such bias. We verify the validity of PDD and the effectiveness of influence estimation through experiments on real-world datasets. Finally, we also demonstrate how the proposed framework could be used for debiasing GNNs. Open-source code can be found at https://github.com/yushundong/BIND.</p> </li> </ol> <p>2. Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network.(1) Tong Li, Jiale Deng, Yanyan Shen, Luyu Qiu, Yongxiang Huang, Caleb Chen Cao. [paper]</p> <ol> <li> <p>Abstract</p> <p>Heterogeneous graph neural networks (HGNs) are prominent approaches to node classification tasks on heterogeneous graphs. Despite the superior performance, insights about the predictions made from HGNs are obscure to humans. Existing explainability techniques are mainly proposed for GNNs on homogeneous graphs. They focus on highlighting salient graph objects to the predictions whereas the problem of how these objects affect the predictions remains unsolved. Given heterogeneous graphs with complex structures and rich semantics, it is imperative that salient objects can be accompanied with their influence paths to the predictions, unveiling the reasoning process of HGNs. In this paper, we develop xPath, a new framework that provides fine-grained explanations for black-box HGNs specifying a cause node with its influence path to the target node. In xPath, we differentiate the influence of a node on the prediction w.r.t. every individual influence path, and measure the influence by perturbing graph structure via a novel graph rewiring algorithm. Furthermore, we introduce a greedy search algorithm to find the most influential fine-grained explanations efficiently. Empirical results on various HGNs and heterogeneous graphs show that xPath yields faithful explanations efficiently, outperforming the adaptations of advanced GNN explanation approaches.</p> </li> </ol> <p>3. Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery.(1) Yunchao Liu, Yu Wang, Oanh Vu, Rocco Moretti, Bobby Bodenheimer, Jens Meiler, Tyler Derr. [paper]</p> <ol> <li> <p>Abstract</p> <p>In computer-aided drug discovery, quantitative structure activity relation models are trained to predict biological activity from chemical structure. Despite the recent success of applying graph neural network to this task, important chemical information such as molecular chirality is ignored. To fill this crucial gap, we propose Molecular-Kernel Graph NeuralNetwork (MolKGNN) for molecular representation learning, which features SE(3)-/conformation invariance, chirality-awareness, and interpretability. For our MolKGNN, we first design a molecular graph convolution to capture the chemical pattern by comparing the atom's similarity with the learnable molecular kernels. Furthermore, we propagate the similarity score to capture the higher-order chemical pattern. To assess the method, we conduct a comprehensive evaluation with nine well-curated datasets spanning numerous important drug targets that feature realistic high class imbalance and it demonstrates the superiority of MolKGNN over other graph neural networks in computer-aided drug discovery. Meanwhile, the learned kernels identify patterns that agree with domain knowledge, confirming the pragmatic interpretability of this approach. Our code and supplementary material are publicly available at https://github.com/meilerlab/MolKGNN.</p> </li> </ol> <p>4. Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis.(1) Han Xuanyuan, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, Pietro Li\u00f2. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2022","title":"Year 2022","text":"<p>1. KerGNNs: Interpretable Graph Neural Networks with Graph Kernels.(1) Aosong Feng, Chenyu You, Shiqiang Wang, Leandros Tassiulas. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.</p> </li> </ol> <p>2. Interpretable Neural Subgraph Matching for Graph Retrieval.(1) Indradyumna Roy, Venkata Sai Baba Reddy Velugoti, Soumen Chakrabarti, Abir De. [paper]</p> <ol> <li> <p>Abstract</p> <p>Given a query graph and a database of corpus graphs, a graph retrieval system aims to deliver the most relevant corpus graphs. Graph retrieval based on subgraph matching has a wide variety of applications, e.g., molecular fingerprint detection, circuit design, software analysis, and question answering. In such applications, a corpus graph is relevant to a query graph, if the query graph is (perfectly or approximately) a subgraph of the corpus graph. Existing neural graph retrieval models compare the node or graph embeddings of the query-corpus pairs, to compute the relevance scores between them. However, such models may not provide edge consistency between the query and corpus graphs. Moreover, they predominantly use symmetric relevance scores, which are not appropriate in the context of subgraph matching, since the underlying relevance score in subgraph search should be measured using the partial order induced by subgraph-supergraph relationship. Consequently, they show poor retrieval performance in the context of subgraph matching. In response, we propose ISONET, a novel interpretable neural edge alignment formulation, which is better able to learn the edge-consistent mapping necessary for subgraph matching. ISONET incorporates a new scoring mechanism which enforces an asymmetric relevance score, specifically tailored to subgraph matching. ISONET\u2019s design enables it to directly identify the underlying subgraph in a corpus graph, which is relevant to the given query graph. Our experiments on diverse datasets show that ISONET outperforms recent graph retrieval formulations and systems. Additionally, ISONET can provide interpretable alignments between query-corpus graph pairs during inference, despite being trained only using binary relevance labels of whole graphs during training, without any fine-grained ground truth information about node or edge alignments.</p> </li> </ol> <p>3. ProtGNN: Towards Self-Explaining Graph Neural Networks.(1) Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Cheekong Lee. [paper]</p> <ol> <li> <p>Abstract</p> <p>Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2021","title":"Year 2021","text":"<p>1. Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network.(1) Seunghyun Lee, Byung Cheol Song. [paper]</p> <ol> <li> <p>Abstract</p> <p>Knowledge distillation (KD) is one of the most useful techniques for light-weight neural networks. Although neural networks have a clear purpose of embedding datasets into the low-dimensional space, the existing knowledge was quite far from this purpose and provided only limited information. We argue that good knowledge should be able to interpret the embedding procedure. This paper proposes a method of generating interpretable embedding procedure (IEP) knowledge based on principal component analysis, and distilling it based on a message passing neural network. Experimental results show that the student network trained by the proposed KD method improves 2.28% in the CIFAR100 dataset, which is a higher performance than the state-of-the-art (SOTA) method. We also demonstrate that the embedding procedure knowledge is interpretable via visualization of the proposed KD process. The implemented code is available at https://github.com/sseung0703/IEPKT.</p> </li> </ol> <p>2. Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks.(1) Yuhang Yao, Carlee Joe-Wong. [paper]</p> <ol> <li> <p>Abstract</p> <p>We study the problem of clustering nodes in a dynamic graph, where the connections between nodes and nodes' cluster memberships may change over time, e.g., due to community migration. We first propose a dynamic stochastic block model that captures these changes, and a simple decay-based clustering algorithm that clusters nodes based on weighted connections between them, where the weight decreases at a fixed rate over time. This decay rate can then be interpreted as signifying the importance of including historical connection information in the clustering. However, the optimal decay rate may differ for clusters with different rates of turnover. We characterize the optimal decay rate for each cluster and propose a clustering method that achieves almost exact recovery of the true clusters. We then demonstrate the efficacy of our clustering algorithm with optimized decay rates on simulated graph data. Recurrent neural networks (RNNs), a popular algorithm for sequence learning, use a similar decay-based method, and we use this insight to propose two new RNN-GCN (graph convolutional network) architectures for semi-supervised graph clustering. We finally demonstrate that the proposed architectures perform well on real data compared to state-of-the-art graph clustering algorithms.</p> </li> </ol>"},{"location":"topconf/NeurIPS/","title":"Paper Index of XAI for GNNs in NeurIPS","text":""},{"location":"topconf/NeurIPS/#year-2023","title":"Year 2023","text":"<p>1. TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery.(1) Jialin Chen, Rex Ying. [paper]</p> <ol> <li> <p>Abstract</p> <p>Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.</p> </li> </ol> <p>2. V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs.(1) Senzhang Wang, Jun Yin, Chaozhuo Li, Xing Xie, Jianxin Wang. [paper]</p> <ol> <li> <p>Abstract</p> <p>GNN explanation method aims to identify an explanatory subgraph which contains the most informative components of the full graph. However, a major limitation of existing GNN explainers is that they are not robust to the structurally corrupted graphs, e.g., graphs with noisy or adversarial edges. On the one hand, existing GNN explainers mostly explore explanations based on either the raw graph features or the learned latent representations, both of which can be easily corrupted. On the other hand, the corruptions in graphs are irregular in terms of the structural properties, e.g., the size or connectivity of graphs, which makes the rigorous constraints used by previous GNN explainers unfeasible. To address these issues, we propose a robust GNN explainer called V-InfoR. Specifically, a robust graph representation extractor, which takes insights of variational inference, is proposed to infer the latent distribution of graph representations. Instead of directly using the corrupted raw features or representations of each single graph, we sample the graph representations from the inferred distribution for the downstream explanation generator, which can effectively eliminate the minor corruption. We next formulate the explanation exploration as a graph information bottleneck (GIB) optimization problem. As a more general method that does not need any rigorous structural constraints, our GIB-based method can adaptively capture both the regularity and irregularity of the severely corrupted graphs for explanation. Extensive evaluations on both synthetic and real-world datasets indicate that V-InfoR significantly improves the GNN explanation performance for the structurally corrupted graphs. Code and dataset are available at https://anonymous.4open.science/r/V-InfoR-EF88.</p> </li> </ol> <p>3. Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks.(1) Jun Yin, Chaozhuo Li, Hao Yan, Jianxun Lian, Senzhang Wang. [paper]</p> <ol> <li> <p>Abstract</p> <p>Intrinsic interpretable graph neural networks aim to provide transparent predictions by identifying the influential fraction of the input graph that guides the model prediction, i.e., the explanatory subgraph. However, current interpretable GNNs mostly are dataset-specific and hard to generalize to different graphs. A more generalizable GNN interpretation model which can effectively distill the universal structural patterns of different graphs is until-now unexplored. Motivated by the great success of recent pre-training techniques, we for the first time propose the Pre-training Interpretable Graph Neural Network (\\(\\pi\\)-GNN) to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations. Specifically, we introduce a structural pattern learning module to extract diverse universal structure patterns and integrate them together to comprehensively represent the graphs of different types. Next, a hypergraph refining module is proposed to identify the explanatory subgraph by incorporating the universal structure patterns with local edge interactions. Finally, the task-specific predictor is cascaded with the pre-trained \\(\\pi\\)-GNN model and fine-tuned over downstream tasks. Extensive experiments demonstrate that \\(\\pi\\)-GNN significantly surpasses the leading interpretable GNN baselines with up to 9.98\\% interpretation improvement and 16.06\\% classification accuracy improvement. Meanwhile, \\(\\pi\\)-GNN pre-trained on graph classification task also achieves the top-tier interpretation performance on node classification task, which further verifies its promising generalization performance among different downstream tasks. Our code and datasets are available at https://anonymous.4open.science/r/PI-GNN-F86C.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2022","title":"Year 2022","text":"<p>1. GStarX: Explaining Graph Neural Networks with Structure-Aware Cooperative Games.(1) Shichang Zhang, Yozen Liu, Neil Shah, Yizhou Sun. [paper]</p> <ol> <li> <p>Abstract</p> <p>Explaining machine learning models is an important and increasingly popular area of research interest. The Shapley value from game theory has been proposed as a prime approach to compute feature importance towards model predictions on images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for GNN explanation, where the task is to identify the most important subgraph and constituent nodes for GNN predictions. We claim that the Shapley value is a non-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Specifically, we define a scoring function based on a new structure-aware value from the cooperative game theory proposed by Hamiache and Navarro (HN). When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, resembling message passing in GNNs, so that node importance scores reflect not only the node feature importance, but also the node structural roles. We demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity over strong baselines on chemical graph property prediction and text graph sentiment classification. Code: https://github.com/ShichangZh/GStarX.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2021","title":"Year 2021","text":"<p>1. Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks.(1) Pascal Esser, Leena Chennuru Vankadara, Debarghya Ghoshdastidar. [paper]</p> <ol> <li> <p>Abstract</p> <p>In recent years, several results in the supervised learning setting suggested that classical statistical learning-theoretic measures, such as VC dimension, do not adequately explain the performance of deep learning models which prompted a slew of work in the infinite-width and iteration regimes. However, there is little theoretical explanation for the success of neural networks beyond the supervised setting. In this paper we argue that, under some distributional assumptions, classical learning-theoretic measures can sufficiently explain generalization for graph neural networks in the transductive setting. In particular, we provide a rigorous analysis of the performance of neural networks in the context of transductive inference, specifically by analysing the generalisation properties of graph convolutional networks for the problem of node classification. While VC-dimension does result in trivial generalisation error bounds in this setting as well, we show that transductive Rademacher complexity can explain the generalisation properties of graph convolutional networks for stochastic block models. We further use the generalisation error bounds based on transductive Rademacher complexity to demonstrate the role of graph convolutions and network architectures in achieving smaller generalisation error and provide insights into when the graph structure can help in learning. The findings of this paper could re-new the interest in studying generalisation in neural networks in terms of learning-theoretic measures, albeit in specific problems.</p> </li> </ol> <p>2. Reinforcement Learning Enhanced Explainer for Graph Neural Networks.(1)  Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, Dongsheng Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph neural networks (GNNs) have recently emerged as revolutionary technologies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most influential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.</p> </li> </ol> <p>3. Towards Multi-Grained Explainability for Graph Neural Networks.(1) Xiang Wang, Yingxin Wu, An Zhang, Xiangnan He, Tat-Seng Chua2. [paper]</p> <ol> <li> <p>Abstract</p> <p>When a graph neural network (GNN) made a prediction, one raises question about explainability: \u201cWhich fraction of the input graph is most in\ufb02uential to the model\u2019s decision?\u201d Producing an answer requires understanding the model\u2019s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the \ufb02exibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and \ufb01ne-tuning idea to develop our explainer and generate multi-grained explanations. Speci\ufb01cally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the \ufb01ne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classi\ufb01cation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2020","title":"Year 2020","text":"<p>1. Parameterized Explainer for Graph Neural Network.(1) Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang. [paper]</p> <ol> <li> <p>Abstract</p> <p>Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging open problem. The leading method mainly addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has a better generalization power and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7\\% relative improvement in AUC on explaining graph classification over the leading baseline.</p> </li> </ol> <p>2. PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks.(1) Minh Vu, My T. Thai. [paper]</p> <ol> <li> <p>Abstract</p> <p>In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.</p> </li> </ol>"}]}