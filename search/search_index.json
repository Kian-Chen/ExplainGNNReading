{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stemmer"]},"docs":[{"location":"","title":"Paper Index of XAI for GNNs","text":"<p>Papers about the explainability of GNNs</p>"},{"location":"#surveys","title":"Surveys","text":"<ol> <li>[Proceedings of the IEEE 24] Trustworthy Graph Neural Networks: Aspects, Methods and Trends paper</li> <li>[Preprint 24] Graph-Based Explainable AI: A Comprehensive Survey paper</li> <li>[Arixv 23] A Survey on Explainability of Graph Neural Networks paper</li> <li>[ACM computing survey] A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation, and Research Challenges paper</li> <li>[TPAMI 22]Explainability in graph neural networks: A taxonomic survey. Yuan Hao, Yu Haiyang, Gui Shurui, Ji Shuiwang. paper</li> <li>[Arxiv 22]A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics paper</li> <li>[Arxiv 22] A Survey of Trustworthy Graph Learning: Reliability, Explainability, and Privacy Protection paper</li> <li>[Big Data 2022]A Survey of Explainable Graph Neural Networks for Cyber Malware Analysis paper</li> <li>[Arxiv 23] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainabilitypaper</li> <li>[Arxiv 22] Explaining the Explainers in Graph Neural Networks: a Comparative Study paper</li> <li>[Book 23] Generative Explanation for Graph Neural Network: Methods and Evaluation paper</li> </ol>"},{"location":"#platforms","title":"Platforms","text":"<ol> <li>PyTorch Geometric [Document] [Blog]</li> <li>DIG: A Turnkey Library for Diving into Graph Deep Learning Research paper Code</li> <li>GraphXAI: Evaluating Explainability for Graph Neural Networks paper Code</li> <li>GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks paper Code</li> <li>GNNExplainer and PGExplainer paper Code</li> <li>BAGEL: A Benchmark for Assessing Graph Neural Network Explanations [paper]Code</li> </ol>"},{"location":"#most-influential-papers-selected-by-cogdl","title":"Most Influential Papers selected by Cogdl","text":"<ol> <li>Explainability in graph neural networks: A taxonomic survey. Yuan Hao, Yu Haiyang, Gui Shurui, Ji Shuiwang. ARXIV 2020. paper</li> <li>Gnnexplainer: Generating explanations for graph neural networks. Ying Rex, Bourgeois Dylan, You Jiaxuan, Zitnik Marinka, Leskovec Jure. NeurIPS 2019. paper code</li> <li>Explainability methods for graph convolutional neural networks. Pope Phillip E, Kolouri Soheil, Rostami Mohammad, Martin Charles E, Hoffmann Heiko. CVPR 2019.paper</li> <li>Parameterized Explainer for Graph Neural Network. Luo Dongsheng, Cheng Wei, Xu Dongkuan, Yu Wenchao, Zong Bo, Chen Haifeng, Zhang Xiang. NeurIPS 2020. paper code</li> <li>Xgnn: Towards model-level explanations of graph neural networks. Yuan Hao, Tang Jiliang, Hu Xia, Ji Shuiwang. KDD 2020. paper. </li> <li>Evaluating Attribution for Graph Neural Networks. Sanchez-Lengeling Benjamin, Wei Jennifer, Lee Brian, Reif Emily, Wang Peter, Qian Wesley, McCloskey Kevin, Colwell  Lucy, Wiltschko Alexander. NeurIPS  2020.paper</li> <li>PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks. Vu Minh, Thai My T.. NeurIPS  2020.paper</li> <li>Explanation-based Weakly-supervised Learning of Visual Relations with Graph Networks. Federico Baldassarre and Kevin Smith and Josephine Sullivan and Hossein Azizpour. ECCV 2020.paper</li> <li>GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media. Lu, Yi-Ju and Li, Cheng-Te. ACL 2020.paper</li> <li>On Explainability of Graph Neural Networks via Subgraph Explorations. Yuan Hao, Yu Haiyang, Wang Jie, Li Kang, Ji Shuiwang. ICML 2021.paper</li> </ol>"},{"location":"#year-2024","title":"Year 2024","text":"<ol> <li>[NeurIPS 24] RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task [paper]</li> <li>[NeurIPS 24] GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules[paper]</li> <li>[ICML 24] Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks[paper]</li> <li>[ICML 24] Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks [paper]</li> <li>[ICML 24] Graph Neural Network Explanations are Fragile [paper]</li> <li>[ICML 24] How Interpretable Are Interpretable Graph Neural Networks? [paper]</li> <li>[ICML 24] Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation[paper]</li> <li>[ICML 24] Explaining Graph Neural Networks via Structure-aware Interaction Index [paper]</li> <li>[ICML 24] EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time [paper]</li> <li>[ICLR 24] GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks [paper]</li> <li>[ICLR 24] GOAt: Explaining Graph Neural Networks via Graph Output Attribution [paper]</li> <li>[ICLR 24] Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks [paper]</li> <li>[ICLR 24] GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking [paper]</li> <li>[ICLR 24] UNR-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models [paper]</li> <li>[TPAMI 24] Towards Inductive and Efficient Explanations for Graph Neural Networks[paper]</li> <li>[TMLR 24] InduCE: Inductive Counterfactual Explanations for Graph Neural Networks [paper]</li> <li>[PLDI 24] PL4XGL: A Programming Language Approach to Explainable Graph Learning[paper]</li> <li>[Usenix Security 24] INSIGHT: Attacking Industry-Adopted Learning Resilient Logic Locking Techniques Using Explainable Graph Neural Network[paper]</li> <li>[SIGMOD 24]View-based Explanations for Graph Neural Networks [paper]</li> <li>[Thesis UCLA] Explainable Artificial Intelligence for Graph Data[paper]</li> <li>[Thesis UVA] Algorithmic Fairness in Graph Machine Learning: Explanation, Optimization, and Certification[paper]</li> <li>[KDD 24] SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative Mask Learning[paper]</li> <li>[KDD 24] Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck[paper]</li> <li>[KDD 24] Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks[paper]</li> <li>[ICDE 24] Generating Robust Counterfactual Witnesses for Graph Neural Networks [paper]</li> <li>[ICDE 24] SES: Bridging the Gap Between Explainability and Prediction of Graph Neural Networks[paper]</li> <li>[ICSE 24] Coca: Improving and Explaining Graph Neural Network-Based Vulnerability Detection Systems[paper]</li> <li>[AAAI 24] Generating Diagnostic and Actionable Explanations for Fair Graph Neural Networks [paper]</li> <li>[AAAI 24] Stratifed GNN Explanations through Sufficient Expansion[paper]</li> <li>[AAAI 24] Factorized Explainer for Graph Neural Networks[paper]</li> <li>[AAAI 24] Self-Interpretable Graph Learning with Sufficient and Necessary Explanations</li> <li>[AAAI 24] Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder [paper]</li> <li>[AISTATS 24] Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process [paper]</li> <li>[WWW 24] Game-theoretic Counterfactual Explanation for Graph Neural Networks [paper]</li> <li>[WWW 24] EXGC: Bridging Efficiency and Explainability in Graph Condensation[paper]</li> <li>[WWW 24] Adversarial Mask Explainer for Graph Neural Networks [paper]</li> <li>[WWW 24] Globally Interpretable Graph Learning via Distribution Matching[paper]</li> <li>[WWW 24] GNNShap: Scalable and Accurate GNN Explanation using Shapley Values [paper]</li> <li>[TAI 24] Learning Counterfactual Explanation of Graph Neural Networks via Generative Flow Network[paper]</li> <li>[IEEE TMI 24] Multi-Modal Diagnosis of Alzheimer\u2019s Disease using Interpretable Graph Convolutional Networks[paper]</li> <li>[IEEE IoT 24] EXVul: Toward Effective and Explainable Vulnerability Detection for IoT Devices[paper]</li> <li>[ECML/PKDD 24] Towards Few-shot Self-explaining Graph Neural Networks[paper]</li> <li>[SDM 24] XGExplainer: Robust Evaluation-based Explanation for Graph Neural Networks[paper]</li> <li>[DASFAA 24] Multi-objective Graph Neural Network Explanatory Model with Local and Global Information Preservation[paper]</li> <li>[ISSTA 2024] Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation [paper]</li> <li>[KBS 24] Shapley-based graph explanation in embedding space[paper]</li> <li>[KBS 24] GEAR: Learning graph neural network explainer via adjusting gradients[paper]</li> <li>[IEEE TNSM 24] Ensemble Graph Attention Networks for Cellular Network Analytics: From Model Creation to Explainability[paper]</li> <li>[IEEE TNSE 24] GAXG: A Global and Self-adaptive Optimal Graph Topology Generation Framework for Explaining Graph Neural Networks[paper]</li> <li>[IEEE TETCI 24] GF-LRP: A Method for Explaining Predictions Made by Variational Graph Auto-Encoders[paper]</li> <li>[AAAI workshop] Semi-Supervised Graph Representation Learning with Human-centric Explanation for Predicting Fatty Liver Disease[paper]</li> <li>[xAI 24] Global Concept Explanations for Graphs by Contrastive Learning [paper]</li> <li>[Arxiv 24.09] PAGE: Parametric Generative Explainer for Graph Neural Network [paper]</li> <li>[Arxiv 24.09] Higher Order Structures For Graph Explanations [paper]</li> <li>[Arxiv 24.08] SE-SGformer: A Self-Explainable Signed Graph Transformer for Link Sign Prediction[paper]</li> <li>[Preprint 24.08] CIDER: Counterfactual-Invariant Diffusion-based GNN Explainer for Causal Subgraph Inference[paper]</li> <li>[Arxiv 24.07] LLMExplainer: Large Language Model based Bayesian Inference for Graph Explanation Generation[paper]</li> <li>[Arxiv 24.07] xAI-Drop: Don't Use What You Cannot Explain[paper]</li> <li>[Arxiv 24.07] Explaining Graph Neural Networks for Node Similarity on Graphs[paper]</li> <li>[Arxiv 24.07] SLInterpreter: An Exploratory and Iterative Human-AI Collaborative System for GNN-based Synthetic Lethal Prediction[paper]</li> <li>[Arxiv 24.07] Graph Neural Network Causal Explanation via Neural Causal Models[paper]</li> <li>[Arxiv 24.06] GNNAnatomy: Systematic Generation and Evaluation of Multi-Level Explanations for Graph Neural Networks[paper]</li> <li>[Arxiv 24.06] On GNN explanability with activation rules[paper]</li> <li>[Arxiv 24.06] Revisiting Attention Weights as Interpretations of Message-Passing Neural Networks[paper]</li> <li>[Arxiv 24.05] SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs[paper]</li> <li>[Arxiv 24.06] Towards Understanding Sensitive and Decisive Patterns in Explainable AI: A Case Study of Model Interpretation in Geometric Deep Learning[paper]</li> <li>[Arxiv 24.06] Explainable Graph Neural Networks Under Fire [paper]</li> <li>[Arxiv 24.06] Explainable AI Security: Exploring Robustness of Graph Neural Networks to Adversarial Attacks [paper]</li> <li>[Arxiv 24.06] Perks and Pitfalls of Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs [paper]</li> <li>[Arxiv 24.05] Utilizing Description Logics for Global Explanations of Heterogeneous Graph Neural Networks [paper]</li> <li>[Arxiv 24.05] MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation [paper]</li> <li>[Arxiv 24.05] Detecting Complex Multi-step Attacks with Explainable Graph Neural Network [paper]</li> <li>[Arxiv 24.05] SynHING: Synthetic Heterogeneous Information Network Generation for Graph Learning and Explanation[paper]</li> <li>[Preprint 24.05] Explainable Graph Neural Networks: An Application to Open Statistics Knowledge Graphs for Estimating House Prices [paper]</li> <li>[Arxiv 24.04] Superior Polymeric Gas Separation Membrane Designed by Explainable Graph Machine Learning [paper]</li> <li>[Arxiv 24.04] Improving the interpretability of GNN predictions through conformal-based graph sparsification [paper]</li> <li>[Arxiv 24.03] GreeDy and CoDy: Counterfactual Explainers for Dynamic Graph[paper]</li> <li>[Arxiv 24.03] Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation[paper]</li> <li>[Arxiv 24.03] Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs[paper]</li> <li>[Arixv 24.03] Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining of Explanations[paper]</li> <li>[Arxiv 24.03] Explainable Graph Neural Networks for Observation Impact Analysis in Atmospheric State Estimation[paper]</li> <li>[Arxiv 24.02] PAC Learnability under Explanation-Preserving Graph Perturbations[paper]</li> <li>[Arxiv 24.02] Explainable Global Wildfire Prediction Models using Graph Neural Networks[paper]</li> <li>[Arxiv 24.02] Incorporating Retrieval-based Causal Learning with Information Bottlenecks for Interpretable Graph Neural Networks[paper]</li> <li>[Arxiv 24.01] On Discprecncies between Perturbation Evaluations of Graph Neural Network Attributions[paper]</li> <li>[ASP=DAC 24] LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking[paper]</li> <li>[Biorxiv 24] Community-aware explanations in knowledge graphs with XP-GNN[paper]</li> <li>[ISCV 24] Adaptive Subgraph Feature Extraction for Explainable Multi-Modal Learning[paper]</li> <li>[IJCNN] Explanations for Graph Neural Networks using A Game-theoretic Value[paper]</li> <li>[Neurocomputing] GeoExplainer: Interpreting Graph Convolutional Networks with geometric masking[paper]</li> <li>[Technologies] Explainable Graph Neural Networks: An Application to Open Statistics Knowledge Graphs for Estimating House Prices[paper]</li> <li>[Reliab. Eng. Syst. Saf.] Causal intervention graph neural network for fault diagnosis of complex industrial processes[paper]</li> <li>[Frontiers in big data] Global explanation supervision for Graph Neural Networks[paper]</li> <li>[Information and Software Technology] Graph-based explainable vulnerability prediction[paper]</li> <li>[Information Systems] Heterogeneous graph neural networks for fraud detection and explanation in supply chain finance[paper]</li> <li>[Information Procs. &amp; Mana.] Towards explaining graph neural networks via preserving prediction ranking and structural dependency[paper]</li> <li>[Applied Energy]  Explainable Spatio-Temporal Graph Neural Networks for multi-site photovoltaic energy production [paper]</li> <li>[PAKDD 24] Random Mask Perturbation Based Explainable Method of Graph Neural Networks [paper]</li> <li>[Computational Materials Science] Graph isomorphism network for materials property prediction along with explainability analysis[paper]</li> <li>[NN 24] Explanatory subgraph attacks against Graph Neural Networks[paper]</li> <li>[NN 24] GRAM: An interpretable approach for graph anomaly detection using gradient attention maps[paper]</li> <li>[Neural Networks 24] CI-GNN: A Granger Causality-Inspired Graph Neural Network for Interpretable Brain Network-Based Psychiatric Diagnosis [paper]</li> <li>[NeuroImage 24] BPI-GNN: Interpretable brain network-based psychiatric diagnosis and subtyping[paper]</li> <li>[PAKDD 24] Toward Interpretable Graph Classification via Concept-Focused Structural Correspondence [paper]</li> <li>[MedRxiv 24] An Interpretable Population Graph Network to Identify Rapid Progression of Alzheimer\u2019s Disease Using UK Biobank[paper]</li> <li>[IEEE TDSC 24] TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support [paper]</li> <li>[IEEE Transactions] IEEE Transactions on Computational Social Systems[paper]</li> <li>[Journal of Physics] Explainer on GNN-based segmentation networks[paper]</li> <li>[Energy and AI] Electricity demand forecasting at distribution and household levels using explainable causal graph neural network [paper]</li> </ol>"},{"location":"#year-2023","title":"Year 2023","text":"<ol> <li>[NeurIPS 23] Interpretable Graph Networks Formulate Universal Algebra Conjectures[paper]</li> <li>[NeurIPS 23] SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanation [paper]</li> <li>[NeurIPS 23] Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks[paper]</li> <li>[NeurIPS 23] D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion [paper]</li> <li>[NeurIPS 23] TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery [paper]</li> <li>[NeurIPS 23] V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs [paper]</li> <li>[NeurIPS 23] Towards Self-Interpretable Graph-Level Anomaly Detection [paper]</li> <li>[NeurIPS 23] Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis [paper]</li> <li>[NeurIPS 23] Interpretable Prototype-based Graph Information Bottleneck [paper]</li> <li>[ICML 23] Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching [paper]</li> <li>[ICML 23] Relevant Walk Search for Explaining Graph Neural Networks [paper]</li> <li>[ICML 23] Towards Understanding the Generalization of Graph Neural Networks [paper]</li> <li>[ICLR 23] GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks [paper]</li> <li>[ICLR 23] Global Explainability of GNNs via Logic Combination of Learned Concepts [paper]</li> <li>[ICLR 23] Explaining Temporal Graph Models through an Explorer-Navigator Framework [paper]</li> <li>[ICLR 23] DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks [paper]</li> <li>[ICLR 23] Interpretable Geometric Deep Learning via Learnable Randomness Injection [paper]</li> <li>[ICLR 23] A Differential Geometric View and Explainability of GNN on Evolving Graphs [paper]</li> <li>[KDD 23] MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation [paper]</li> <li>[KDD 23] Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation [paper]</li> <li>[KDD 23] Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective[paper]</li> <li>[KDD 23] Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining.[paper]</li> <li>[KDD 23] Shift-Robust Molecular Relational Learning with Causal Substructure [paper]</li> <li>[AAAI 23] Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis [paper]</li> <li>[AAAI 23] On the Limit of Explaining Black-box Temporal Graph Neural Networks [paper]</li> <li>[AAAI 23] Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network [paper]</li> <li>[AAAI 23] Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery [paper]</li> <li>[VLDB 23] HENCE-X: Toward Heterogeneity-agnostic Multi-level Explainability for Deep Graph Networks [paper]</li> <li>[VLDB 23] On Data-Aware Global Explainability of Graph Neural Networks [paper]</li> <li>[AISTATS 23] Distill n' Explain: explaining graph neural networks using simple surrogates [Paper]</li> <li>[AISTATS 23] Probing Graph Representations [paper]</li> <li>[ICDE 23] INGREX: An Interactive Explanation Framework for Graph Neural Networks[paper]</li> <li>[ICDE 23] Jointly Attacking Graph Neural Network and its Explanations [paper]</li> <li>[WWW 23]PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction [paper]</li> <li>[ICDM 23] Limitations of Perturbation-based Explanation Methods for Temporal Graph Neural Networks</li> <li>[ICDM 23] Interpretable Subgraph Feature Extraction for Hyperlink Prediction[paper]</li> <li>[WSDM 23]Interpretable Research Interest Shift Detection with Temporal Heterogeneous Graphs [paper]</li> <li>[WSDM 23]Cooperative Explanations of Graph Neural Networks [paper]</li> <li>[WSDM 23]Towards Faithful and Consistent Explanations for Graph Neural Networks [paper]</li> <li>[WSDM 23] Global Counterfactual Explainer for Graph Neural Networks [paper]</li> <li>[CIKM 23] Explainable Spatio-Temporal Graph Neural Networks [paper]</li> <li>[CIKM 23] DuoGAT: Dual Time-oriented Graph Attention Networks for Accurate, Efficient and Explainable Anomaly Detection on Time-series. [paper]</li> <li>[CIKM 23] Heterogeneous Temporal Graph Neural Network Explainer [paper]</li> <li>[CIKM 23] ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks[paper]</li> <li>[CIKM 23] KG4Ex: An Explainable Knowledge Graph-Based Approach for Exercise Recommendation [paper]</li> <li>[ECML-PKDD 23] ENGAGE: Explanation Guided Data Augmentation for Graph Representation Learning [paper]</li> <li>[TPAMI 23] FlowX: Towards Explainable Graph Neural Networks via Message Flows [paper]</li> <li>[TAI] Prototype-based interpretable graph neural networks. [paper]</li> <li>[TKDE 23] Counterfactual Graph Learning for Anomaly Detection on Attributed Networks [paper]</li> <li>[Scientific Data 23 ] Evaluating explainability for graph neural networks [paper]</li> <li>[Nature Communications 23] Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking [paper]</li> <li>[ACM Computing Surveys 23] A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation [paper]</li> <li>[TIST 23] Faithful and Consistent Graph Neural Network Explanations with Rationale Alignment [paper]</li> <li>[Openreview 23] STExplainer: Global Explainability of GNNs via Frequent SubTree Mining [paper]</li> <li>[GLFrontiers 23] Everybody Needs a Little HELP: Explaining Graphs via Hierarchical Concepts [paper]</li> <li>[Openreview 23] Iterative Graph Neural Network Enhancement Using Explanations [paper]</li> <li>[Openreview 23] Interpretable and Convergent Graph Neural Network Layers at Scale [paper]</li> <li>[NeurIPS 2023 Workshop XAIA] GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Networks Explanations [paper]</li> <li>[NeurIPS 2023 Workshop XAIA] On the Consistency of GNN Explainability Methods [paper]</li> <li>[Arxiv 23] Evaluating Neighbor Explainability for Graph Neural Networks [paper]</li> <li>[Arxiv 23] DyExplainer: Explainable Dynamic Graph Neural Networks [paper]</li> <li>[Arxiv 23] Explainability-Based Adversarial Attack on Graphs Through Edge Perturbation[paper]</li> <li>[AICS 23] A subgraph interpretation generative model for knowledge graph link prediction based on uni-relation transformation [paper]</li> <li>[GUT 23] Screening of normal endoscopic large bowel biopsies with interpretable graph learning: a retrospective study [paper]</li> <li>[PR 2023] Towards self-explainable graph convolutional neural network with frequency adaptive inception [paper]</li> <li>[MLG 2023] Understanding how explainers work in graph neural networks [paper]</li> <li>[MLG 2023] Graph Model Explainer Tool [paper]</li> <li>[Information Science 23] Robust explanations for graph neural network with neuron explanation component [paper]</li> <li>[Recsys 23] Explainable Graph Neural Network Recommenders; Challenges and Opportunities [paper]</li> <li>[xAI  23] Counterfactual Explanations for Graph Classification Through the Lenses of Density [paper]</li> <li>[XAI 23] Evaluating Link Prediction Explanations for Graph Neural Networks [[paper]](https://arxiv.org/abs/2308.01682</li> <li>[xAI 23] XInsight: Revealing Model Insights for GNNs with Flow-based Explanations [paper]</li> <li>[xAI 23] Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies [paper]</li> <li>[xAI 23] MEGAN: Multi Explanation Graph Attention Network [paper]</li> <li>[XKDD 23] Game Theoretic Explanations for Graph Neural Networks [paper]</li> <li>[XKDD 23] From Black Box to Glass Box: Evaluating Faithfulness of Process Predictions with GCNNs [paper]</li> <li>[IJCNN 23] MEGA: Explaining Graph Neural Networks with Network Motifs [paper]</li> <li>[LOG Poster 23] On the Robustness of Post-hoc GNN Explainers to Label Noise [paper]</li> <li>[LOG Poster 23] How Faithful are Self-Explainable GNNs? [paper]</li> <li>[LOG Poster 23] Explaining Link Predictions in Knowledge Graph Embedding Models with Influential Examples [paper]</li> <li>[Bioriv 23] Building explainable graph neural network by sparse learning for the drug-protein binding prediction [paper]</li> <li>[ICAID 2023] Explanations for Graph Neural Networks via Layer Analysis. [paper]</li> <li>[ECAI 23] XGBD: Explanation-Guided Graph Backdoor Detection [paper]</li> <li>[IEEE Transactions on Consumer Electronics 23] Human Pose Prediction Using Interpretable Graph Convolutional Network for Smart Home [paper]</li> <li>[KBS 23] KE-X: Towards subgraph explanations of knowledge graph embedding based on knowledge information gain [paper]</li> <li>[ICML workshop 23] Generating Global Factual and Counterfactual Explainer for Molecule under Domain Constraints [paper]</li> <li>[Thesis 23] Developing interpretable graph neural networks for high dimensional feature spaces [paper]</li> <li>[Thesis 23] Evaluation of Explainability Methods on Single-Cell Classification Tasks Using Graph Neural Networks [paper]</li> <li>[Arxiv 23] On the Interplay of Subset Selection and Informed Graph Neural Networks [paper]</li> <li>[ISSTA23] Interpreters for GNN-Based Vulnerability Detection: Are We There Yet? [paper]</li> <li>[ICECAI23] Improved GraphSVX for GNN Explanations Based on Cross Entropy [paper]</li> <li>[ICRA Workshop 23] Towards Semantic Interpretation and Validation of Graph Attention-based Explanations [paper]</li> <li>[Arxiv 23] Graph Neural Network based Log Anomaly Detection and Explanation [paper]</li> <li>[Arxiv 23] Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features [paper]</li> <li>[Thesis 23] Interpretability of Graphical Models [paper]</li> <li>[Bioengineering 2023] Personalized Explanations for Early Diagnosis of Alzheimer's Disease Using Explainable Graph Neural Networks with Population Graphs [paper]</li> <li>[BDSC 2023] MDC: An Interpretable GNNs Method Based on Node Motif Degree and Graph Diffusion Convolution [[paper]] (https://link.springer.com/chapter/10.1007/978-981-99-3925-1_24)</li> <li>[Information Science 2023] Explainability techniques applied to road traffic forecasting using Graph Neural Network models [paper]</li> <li>[Arxiv 23] Efficient GNN Explanation via Learning Removal-based Attribution [paper]</li> <li>[Arxiv 23] Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity [paper]</li> <li>[ICLR Tiny 23] Message-passing selection: Towards interpretable GNNs for graph classification [paper]</li> <li>[ICLR Tiny 23] Revisiting CounteRGAN for Counterfactual Explainability of Graphs [paper]</li> <li>[MICCAI Workshop 23] IA-GCN: Interpretable Attention based Graph Convolutional Network for Disease prediction [paper]</li> <li>[Arxiv 23] Robust Ante-hoc Graph Explainer using Bilevel Optimization [paper]</li> <li>[GRADES &amp; NDA'23] A Demonstration of Interpretability Methods for Graph Neural Networks [paper]</li> <li>[Arxiv 23] Self-Explainable Graph Neural Networks for Link Prediction [paper]</li> <li>[ChemRxiv 23] Interpreting Graph Neural Networks with Myerson Values for Cheminformatics Approaches [paper]</li> <li>[Neural Networks 23] Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness [paper]</li> <li>[ICASSP 23] Towards a More Stable and General Subgraph Information Bottleneck [paper]</li> <li>[ESANN 23] Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability [Paper]</li> <li>[IEEE Access] Generating Real-Time Explanations for GNNs via Multiple Specialty Learners and Online Knowledge Distillation [Paper]</li> <li>[IEEE Access] Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions [paper]</li> <li>[Journal of Software 23] A Slice-level vulnerability detection and interpretation method based on graph neural network [paper]</li> <li>[Automation in Construction 23] Learning from explainable data-driven tunneling graphs: A spatio-temporal graph convolutional network for clogging detection [paper]</li> <li>[Briefings in Bioinformatics] Predicting molecular properties based on the interpretable graph neural network with multistep focus mechanism [paper]</li> <li>[Briefings in Bioinformatics] Identification of vital chemical information via visualization of graph neural networks [paper]</li> <li>[Bioinformatics 23] Explainable Multilayer Graph Neural Network for Cancer Gene Prediction [paper]</li> <li>[ICLR Workshop 23] GCI: A Graph Concept Interpretation Framework [paper]</li> <li>[Arxiv 23] Structural Explanations for Graph Neural Networks using HSIC [paper]</li> <li>[Internet of Things 23] XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics [paper]</li> <li>[JOS23] A Generic Explaining &amp; Locating Method for Malware Detection based on Graph Neural Networks [paper]</li> </ol>"},{"location":"#year-2022","title":"Year 2022","text":"<ol> <li>[NeurIPS 22] GStarX:Explaining Graph-level Predictions with Communication Structure-Aware Cooperative Games [paper]</li> <li>[NeurIPS 22] Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure [paper]</li> <li>[NeurIPS 22] Task-Agnostic Graph Neural Explanations [paper]</li> <li>[NeurIPS 22] CLEAR: Generative Counterfactual Explanations on Graphs[paper]</li> <li>[ICML 22] Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism [paper]</li> <li>[ICLR 22] DEGREE: Decomposition Based Explanation for Graph Neural Networks [paper]</li> <li>[ICLR 22] Explainable GNN-Based Models over Knowledge Graphs [paper]</li> <li>[ICLR 22] Discovering Invariant Rationales for Graph Neural Networks [paper]</li> <li>[KDD 22] On Structural Explanation of Bias in Graph Neural Networks [paper]</li> <li>[KDD 22] Causal Attention for Interpretable and Generalizable Graph Classification [paper]</li> <li>[CVPR 22] OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks [paper]</li> <li>[CVPR 22] Improving Subgraph Recognition with Variational Graph Information Bottleneck [paper]</li> <li>[AISTATS 22] Probing GNN Explainers: A Rigorous Theoretical and Empirical Analysis of GNN Explanation Methods [paper]</li> <li>[AISTATS 22] CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks [paper]</li> <li>[TPAMI 22] Differentially Private Graph Neural Networks for Whole-Graph Classification [paper]</li> <li>[TPAMI 22] Reinforced Causal Explainer for Graph Neural Networks [paper]</li> <li>[VLDB 22] xFraud: Explainable Fraud Transaction Detection on Heterogeneous Graphs [paper]</li> <li>[LOG 22]GraphFramEx: Towards Systematic Evaluation of Explainability Methods for Graph Neural Networks [paper]</li> <li>[LOG 22] Towards Training GNNs using Explanation Directed Message Passing [paper]</li> <li>[The Webconf 22] Learning and Evaluating Graph Neural Network Explanations based on Counterfactual and Factual Reasoning [paper]</li> <li>[AAAI 22] Prototype-Based Explanations for Graph Neural Networks [paper]</li> <li>[AAAI 22] KerGNNs: Interpretable Graph Neural Networks with Graph Kernels[paper]</li> <li>[AAAI 22] ProtGNN: Towards Self-Explaining Graph Neural Networks [paper]</li> <li>[IEEE Big Data 22] Trade less Accuracy for Fairness and Trade-off Explanation for GNN [paper]</li> <li>[CIKM 22] GRETEL: A unified framework for Graph Counterfactual Explanation Evaluation [paper]</li> <li>[CIKM 22] GRETEL: Graph Counterfactual Explanation Evaluation Framework[paper]</li> <li>[CIKM 22] A Model-Centric Explainer for Graph Neural Network based Node Classification [paper]</li> <li>[IJCAI 22] What Does My GNN Really Capture? On Exploring Internal GNN Representations [paper]</li> <li>[ECML PKDD 22] Improving the quality of rule-based GNN explanations [paper]</li> <li>[MICCAI 22] Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis [paper]</li> <li>[MICCAI 22] Sparse Interpretation of Graph Convolutional Networks for Multi-modal Diagnosis of Alzheimer\u2019s Disease [paper]</li> <li>[EuroS&amp;P 22] Illuminati: Towards Explaining Graph Neural Networks for Cybersecurity Analysis [paper]</li> <li>[INFOCOM 22] Interpretability Evaluation of Botnet Detection Model based on Graph Neural Network [paper]</li> <li>[GLOBECOM 22] Shapley Explainer - An Interpretation Method for GNNs Used in SDN [paper]</li> <li>[GLOBECOM 22] An Explainer for Temporal Graph Neural Networks [paper]</li> <li>[TKDE 22] Zorro: Valid, Sparse, and Stable Explanations in Graph Neural Networks [paper]</li> <li>[TNNLS 22] Interpretable Graph Reservoir Computing With the Temporal Pattern Attention [paper]</li> <li>[TNNLS22] A Meta-Learning Approach for Training Explainable Graph Neural Networks [paper]</li> <li>[TNNLS 22] Explaining Deep Graph Networks via Input Perturbation [paper]</li> <li>[TNNLS 22] A Meta-Learning Approach for Training Explainable Graph Neural Network [paper]</li> <li>[DMKD 22] On GNN explanability with activation patterns [paper]</li> <li>[KBS 22] EGNN: Constructing explainable graph neural networks via knowledge distillation [paper]</li> <li>[XKDD 22] GREASE: Generate Factual and Counterfactual Explanations for GNN-based Recommendations [paper]</li> <li>[AI 22] Are Graph Neural Network Explainers Robust to Graph Noises? [paper]</li> <li>[BRACIS 22] ConveXplainer for Graph Neural Networks [paper]</li> <li>[GLB 22] An Explainable AI Library for Benchmarking Graph Explainers [paper]</li> <li>[DASFAA 22] On Global Explainability of Graph Neural Networks [paper]</li> <li>[ISBI 22] Interpretable Graph Convolutional Network Of Multi-Modality Brain Imaging For Alzheimer\u2019s Disease Diagnosis [paper]</li> <li>[Bioinformatics] GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks [paper]</li> <li>[Medical Imaging 2022]  Phenotype guided interpretable graph convolutional network analysis of fMRI data reveals changing brain connectivity during adolescence [paper]</li> <li>[NeuroComputing 22] Perturb more, trap more: Understanding behaviors of graph neural networks [paper]</li> <li>[DSN 22] CFGExplainer: Explaining Graph Neural Network-Based Malware Classification from Control Flow Graphs [paper]</li> <li>[IEEE Access 22] Providing Node-level Local Explanation for node2vec through Reinforcement Learning [paper]</li> <li>[Patterns 22] Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction [paper]</li> <li>[Arxiv 22] GRAPHSHAP: Motif-based Explanations for Black-box Graph Classifiers [paper]</li> <li>[IEEE Access 22] Providing Post-Hoc Explanation for Node Representation Learning Models Through Inductive Conformal Predictions [paper]</li> <li>[IEEE 22] Explaining Graph Neural Networks With Topology-Aware Node Selection: Application in Air Quality Inference [paper]</li> <li>[BioRxiv 22] GNN-SubNet: disease subnetwork detection with explainable Graph Neural Networks [paper]</li> <li>[IEEE Robotics and Automation Letters 22] Efficient and Interpretable Robot Manipulation with Graph Neural Networks [paper]</li> <li>[Arxiv 22] Deconfounding to Explanation Evaluation in Graph Neural Networks [paper]</li> <li>[ICCPR 22] GANExplainer: GAN-based Graph Neural Networks Explainer [paper]</li> <li>[Arxiv 22] On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach [paper]</li> <li>[Arxiv 22] Exploring Explainability Methods for Graph Neural Networks [paper]</li> <li>[Arxiv 22] PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks [paper]</li> <li>[Arxiv 22] Toward Multiple Specialty Learners for Explaining GNNs via Online Knowledge Distillation [paper]</li> <li>[Openreview 23] TGP: Explainable Temporal Graph Neural Networks for Personalized Recommendation [paper]</li> <li>[Openreview 23] On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective [paper]</li> <li>[Arxiv 22] L2XGNN: Learning to Explain Graph Neural Networks [paper]</li> <li>[Arxiv 22] Towards Prototype-Based Self-Explainable Graph Neural Network [paper]</li> <li>[Arxiv 22] PGX: A Multi-level GNN Explanation Framework Based on Separate Knowledge Distillation Processes [paper]</li> <li>[Arxiv 22] Explainability in subgraphs-enhanced Graph Neural Networks [paper]</li> <li>[Arxiv 22] Defending Against Backdoor Attack on Graph Neural Network by Explainability [paper]</li> <li>[Arxiv 22] Explaining Dynamic Graph Neural Networks via Relevance Back-propagation [paper]</li> <li>[Arxiv 22] EiX-GNN : Concept-level eigencentrality explainer for graph neural networks [paper]</li> <li>[Arxiv 22] MotifExplainer: a Motif-based Graph Neural Network Explainer [paper]</li> <li>[Arxiv 22] Faithful Explanations for Deep Graph Models [paper]</li> <li>[Arxiv 22] Towards Explanation for Unsupervised Graph-Level Representation Learning [paper]</li> <li>[Arxiv 22] BAGEL: A Benchmark for Assessing Graph Neural Network Explanations [paper]</li> <li>[Arxiv 22] BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck [paper]</li> <li>[Arxiv 22] A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability [paper]</li> <li>[Arxiv 22] Explainability in Graph Neural Networks: An Experimental Survey [paper]</li> <li>[IEEE TSIPN 22] Explainability and Graph Learning from Social Interactions [paper]</li> <li>[Arxiv 22] Cognitive Explainers of Graph Neural Networks Based on Medical Concepts [paper]</li> </ol>"},{"location":"#year-2021","title":"Year 2021","text":"<ol> <li>[NeurIPS 21] SALKG: Learning From Knowledge Graph Explanations for Commonsense Reasoning [paper]</li> <li>[NeurIPS 2021] Reinforcement Learning Enhanced Explainer for Graph Neural Networks [paper]</li> <li>[NeurIPS 2021] Towards Multi-Grained Explainability for Graph Neural Networks [paper]</li> <li>[NeurIPS 2021] Robust Counterfactual Explanations on Graph Neural Networks [paper]</li> <li>[ICML 2021] On Explainability of Graph Neural Networks via Subgraph Explorations[paper]</li> <li>[ICML 2021] Generative Causal Explanations for Graph Neural Networks[paper]</li> <li>[ICML 2021] Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity[paper]</li> <li>[ICML 2021] Automated Graph Representation Learning with Hyperparameter Importance Explanation[paper]</li> <li>[ICLR 2021] Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking[paper]</li> <li>[ICLR 2021] Graph Information Bottleneck for Subgraph Recognition [paper]</li> <li>[KDD 2021] When Comparing to Ground Truth is Wrong: On Evaluating GNN Explanation Methods[paper]</li> <li>[KDD 2021] Counterfactual Graphs for Explainable Classification of Brain Networks [paper]</li> <li>[CVPR 2021] Quantifying Explainers of Graph Neural Networks in Computational Pathology.[paper]</li> <li>[NAACL 2021] Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network. [paper]</li> <li>[AAAI 2021] Motif-Driven Contrastive Learning of Graph Representations [paper]</li> <li>[TPAMI 21] Higher-Order Explanations of Graph Neural Networks via Relevant Walks [paper]</li> <li>[WWW 2021] Interpreting and Unifying Graph Neural Networks with An Optimization Framework [paper]</li> <li>[Genome medicine 21] Explaining decisions of Graph Convolutional Neural Networks: patient-specific molecular subnetworks responsible for metastasis prediction in breast cancer [paper]</li> <li>[IJCKG 21] Knowledge Graph Embedding in E-commerce Applications: Attentive Reasoning, Explanations, and Transferable Rules [paper]</li> <li>[RuleML+RR 21] Combining Sub-Symbolic and Symbolic Methods for Explainability [paper]</li> <li>[PAKDD 21] SCARLET: Explainable Attention based Graph Neural Network for Fake News spreader prediction [paper]</li> <li>[J. Chem. Inf. Model] Coloring Molecules with Explainable Artificial Intelligence for Preclinical Relevance Assessment [paper]</li> <li>[BioRxiv 21] APRILE: Exploring the Molecular Mechanisms of Drug Side Effects with Explainable Graph Neural Networks [paper]</li> <li>[ISM 21] Edge-Level Explanations for Graph Neural Networks by Extending Explainability Methods for Convolutional Neural Networks [paper]</li> <li>[Arxiv 21] Towards the Explanation of Graph Neural Networks in Digital Pathology with Information Flows [paper]</li> <li>[Arxiv 21] SEEN: Sharpening Explanations for Graph Neural Networks using Explanations from Neighborhoods [paper]</li> <li>[Arxiv 21] Preserve, Promote, or Attack? GNN Explanation via Topology Perturbation [paper]</li> <li>[Arxiv 21] Learnt Sparsification for Interpretable Graph Neural Networks [paper]</li> <li>[ICML workshop 21] GCExplainer: Human-in-the-Loop Concept-based Explanations for Graph Neural Networks [paper]</li> <li>[ICML workshop 21] Reliable Graph Neural Network Explanations Through Adversarial Training [paper]</li> <li>[ICML workshop 21] Reimagining GNN Explanations with ideas from Tabular Data [paper]</li> <li>[ICML workshop 21] Towards Automated Evaluation of Explanations in Graph Neural Networks [paper]</li> <li>[ICDM 2021] GNES: Learning to Explain Graph Neural Networks [paper]</li> <li>[ICDM 2021] GCN-SE: Attention as Explainability for Node Classification in Dynamic Graphs [paper]</li> <li>[ICDM 2021] Multi-objective Explanations of GNN Predictions [paper]</li> <li>[CIKM 2021] Towards Self-Explainable Graph Neural Network [paper]</li> <li>[ECML PKDD 2021] GraphSVX: Shapley Value Explanations for Graph Neural Networks [paper]</li> <li>[WiseML 2021] Explainability-based Backdoor Attacks Against Graph Neural Networks [paper]</li> <li>[IJCNN 21] MEG: Generating Molecular Counterfactual Explanations for Deep Graph Networks [paper]</li> <li>[ICCSA 2021] Understanding Drug Abuse Social Network Using Weighted Graph Neural Networks Explainer [paper]</li> <li>[NeSy 21] A New Concept for Explaining Graph Neural Networks [paper]</li> <li>[Information Fusion 21] Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI [paper]</li> <li>[Patterns 21] hcga: Highly Comparative Graph Analysis for network phenotyping [paper]</li> </ol>"},{"location":"#year-2020-and-before","title":"Year 2020 and Before","text":"<ol> <li>[NeurIPS 2020] Parameterized Explainer for Graph Neural Network.[paper]</li> <li>[NeurIPS 2020] PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks [paper]</li> <li>[KDD 2020] XGNN: Towards Model-Level Explanations of Graph Neural Networks [paper]</li> <li>[ACL 2020]GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media. paper</li> <li>[Arxiv 2020] Graph Neural Networks Including Sparse Interpretability [paper]</li> <li>[NeurIPS Workshop 20] Towards explainable message passing networks for predicting carbon dioxide adsorption in metal-organic frameworks [paper]</li> <li>[ICML workstop 2020] Contrastive Graph Neural Network Explanation [paper]</li> <li>[ICML workstop 2020] Towards Explainable Graph Representations in Digital Pathology [paper]</li> <li>[NeurIPS workshop 2020] Explaining Deep Graph Networks with Molecular Counterfactuals [paper]</li> <li>[DataMod 2020] Exploring Graph-Based Neural Networks for Automatic Brain Tumor Segmentation\" [paper]</li> <li>[OpenReview 20] A Framework For Differentiable Discovery Of Graph Algorithms [paper]</li> <li>[OpenReview 20] Causal Screening to Interpret Graph Neural Networks [paper]</li> <li>[Arxiv 20] Understanding Graph Neural Networks from Graph Signal Denoising Perspectives [paper]</li> <li>[Arxiv 20] Understanding the Message Passing in Graph Neural Networks via Power Iteration [paper]</li> <li>[Arxiv 20] xERTE: Explainable Reasoning on Temporal Knowledge Graphs for Forecasting Future Links [paper]</li> <li>[IJCNN 20] GCN-LRP explanation: exploring latent attention of graph convolutional networks] [paper]</li> <li>[CD-MAKE 20] Explain Graph Neural Networks to Understand Weighted Graph Features in Node Classification [paper] </li> <li>[ICDM 19] Scalable Explanation of Inferences on Large Graphs[paper] </li> </ol>"},{"location":"surveys/hal-04660442/hal-04660442/","title":"Graph-Based Explainable AI: A Comprehensive Survey","text":"<p>code</p>"},{"location":"surveys/hal-04660442/hal-04660442/#0-overview","title":"0. Overview","text":"<pre><code>graph TD\n    style A fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style B fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style C fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n    style D fill:#FDE9D9,stroke:#B2925A,stroke-width:2px\n\n    style E fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style F fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style G fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n    style H fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n    style I fill:#E6E6E6,stroke:#BFBFBF,stroke-width:2px\n\n    style J fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style K fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style L fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n\n    A[Explainability on Graphs]\n    B[Scoring] --&gt; E[Gradients]\n    B[Scoring] --&gt; F[Decomposition]\n\n    A --&gt; B[Scoring]\n    A --&gt; C[Extraction]\n    A --&gt; D[Generation]\n\n    C[Extraction] --&gt; G[Sequential Paths]\n    C[Extraction] --&gt; H[Logic Rules]\n    C[Extraction] --&gt; I[Subgraph]\n\n    G[Sequential Paths] --&gt; J[Path Reasoning]\n    H[Logic Rules] --&gt; K[Data Integration]\n    G --&gt; K\n    H[Logic Rules] --&gt; L[Surrogate]\n    I[Subgraph] --&gt;J\n    I[Subgraph] --&gt;K\n    I[Subgraph] --&gt; M[Perturbation]\n    I[Subgraph] --&gt; N[Graph Creation]\n\n    D --&gt; N\n\n    style M fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px\n    style N fill:#DCEEFF,stroke:#91C0F2,stroke-width:2px</code></pre>"},{"location":"topconf/AAAI/","title":"Paper Index of XAI for GNNs in AAAI","text":""},{"location":"topconf/AAAI/#year-2024","title":"Year 2024","text":"<p>1. Factorized Explainer for Graph Neural Networks.(1) Rundong Huang, Farhad Shirani, Dongsheng Luo. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have received increasing attention due to their ability to learn from graph-structured data. To open the black-box of these deep learning models, post-hoc instance-level explanation methods have been proposed to understand GNN predictions. These methods seek to discover substructures that explain the prediction behavior of a trained GNN. In this paper, we show analytically that for a large class of explanation tasks, conventional approaches, which are based on the principle of graph information bottleneck (GIB), admit trivial solutions that do not align with the notion of explainability. Instead, we argue that a modified GIB principle may be used to avoid the aforementioned trivial solutions. We further introduce a novel factorized explanation model with theoretical performance guarantees. The modified GIB is used to analyze the structural properties of the proposed factorized explainer. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness of our proposed factorized explainer.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2023","title":"Year 2023","text":"<p>1. Interpreting Unfairness in Graph Neural Networks via Training Node Attribution.(1) Yushun Dong, Song Wang, Jing Ma, Ninghao Liu, Jundong Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have emerged as the leading paradigm for solving graph analytical problems in various real-world applications. Nevertheless, GNNs could potentially render biased predictions towards certain demographic subgroups. Understanding how the bias in predictions arises is critical, as it guides the design of GNN debiasing mechanisms. However, most existing works overwhelmingly focus on GNN debiasing, but fall short on explaining how such bias is induced. In this paper, we study a novel problem of interpreting GNN unfairness through attributing it to the influence of training nodes. Specifically, we propose a novel strategy named Probabilistic Distribution Disparity (PDD) to measure the bias exhibited in GNNs, and develop an algorithm to efficiently estimate the influence of each training node on such bias. We verify the validity of PDD and the effectiveness of influence estimation through experiments on real-world datasets. Finally, we also demonstrate how the proposed framework could be used for debiasing GNNs. Open-source code can be found at https://github.com/yushundong/BIND.</p> </li> </ol> <p>2. Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network.(1) Tong Li, Jiale Deng, Yanyan Shen, Luyu Qiu, Yongxiang Huang, Caleb Chen Cao. [paper]</p> <ol> <li> <p>Abstract</p> <p>Heterogeneous graph neural networks (HGNs) are prominent approaches to node classification tasks on heterogeneous graphs. Despite the superior performance, insights about the predictions made from HGNs are obscure to humans. Existing explainability techniques are mainly proposed for GNNs on homogeneous graphs. They focus on highlighting salient graph objects to the predictions whereas the problem of how these objects affect the predictions remains unsolved. Given heterogeneous graphs with complex structures and rich semantics, it is imperative that salient objects can be accompanied with their influence paths to the predictions, unveiling the reasoning process of HGNs. In this paper, we develop xPath, a new framework that provides fine-grained explanations for black-box HGNs specifying a cause node with its influence path to the target node. In xPath, we differentiate the influence of a node on the prediction w.r.t. every individual influence path, and measure the influence by perturbing graph structure via a novel graph rewiring algorithm. Furthermore, we introduce a greedy search algorithm to find the most influential fine-grained explanations efficiently. Empirical results on various HGNs and heterogeneous graphs show that xPath yields faithful explanations efficiently, outperforming the adaptations of advanced GNN explanation approaches.</p> </li> </ol> <p>3. Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery.(1) Yunchao Liu, Yu Wang, Oanh Vu, Rocco Moretti, Bobby Bodenheimer, Jens Meiler, Tyler Derr. [paper]</p> <ol> <li> <p>Abstract</p> <p>In computer-aided drug discovery, quantitative structure activity relation models are trained to predict biological activity from chemical structure. Despite the recent success of applying graph neural network to this task, important chemical information such as molecular chirality is ignored. To fill this crucial gap, we propose Molecular-Kernel Graph NeuralNetwork (MolKGNN) for molecular representation learning, which features SE(3)-/conformation invariance, chirality-awareness, and interpretability. For our MolKGNN, we first design a molecular graph convolution to capture the chemical pattern by comparing the atom's similarity with the learnable molecular kernels. Furthermore, we propagate the similarity score to capture the higher-order chemical pattern. To assess the method, we conduct a comprehensive evaluation with nine well-curated datasets spanning numerous important drug targets that feature realistic high class imbalance and it demonstrates the superiority of MolKGNN over other graph neural networks in computer-aided drug discovery. Meanwhile, the learned kernels identify patterns that agree with domain knowledge, confirming the pragmatic interpretability of this approach. Our code and supplementary material are publicly available at https://github.com/meilerlab/MolKGNN.</p> </li> </ol> <p>4. Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis.(1) Han Xuanyuan, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, Pietro Li\u00f2. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2022","title":"Year 2022","text":"<p>1. KerGNNs: Interpretable Graph Neural Networks with Graph Kernels.(1) Aosong Feng, Chenyu You, Shiqiang Wang, Leandros Tassiulas. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.</p> </li> </ol> <p>2. Interpretable Neural Subgraph Matching for Graph Retrieval.(1) Indradyumna Roy, Venkata Sai Baba Reddy Velugoti, Soumen Chakrabarti, Abir De. [paper]</p> <ol> <li> <p>Abstract</p> <p>Given a query graph and a database of corpus graphs, a graph retrieval system aims to deliver the most relevant corpus graphs. Graph retrieval based on subgraph matching has a wide variety of applications, e.g., molecular fingerprint detection, circuit design, software analysis, and question answering. In such applications, a corpus graph is relevant to a query graph, if the query graph is (perfectly or approximately) a subgraph of the corpus graph. Existing neural graph retrieval models compare the node or graph embeddings of the query-corpus pairs, to compute the relevance scores between them. However, such models may not provide edge consistency between the query and corpus graphs. Moreover, they predominantly use symmetric relevance scores, which are not appropriate in the context of subgraph matching, since the underlying relevance score in subgraph search should be measured using the partial order induced by subgraph-supergraph relationship. Consequently, they show poor retrieval performance in the context of subgraph matching. In response, we propose ISONET, a novel interpretable neural edge alignment formulation, which is better able to learn the edge-consistent mapping necessary for subgraph matching. ISONET incorporates a new scoring mechanism which enforces an asymmetric relevance score, specifically tailored to subgraph matching. ISONET\u2019s design enables it to directly identify the underlying subgraph in a corpus graph, which is relevant to the given query graph. Our experiments on diverse datasets show that ISONET outperforms recent graph retrieval formulations and systems. Additionally, ISONET can provide interpretable alignments between query-corpus graph pairs during inference, despite being trained only using binary relevance labels of whole graphs during training, without any fine-grained ground truth information about node or edge alignments.</p> </li> </ol> <p>3. ProtGNN: Towards Self-Explaining Graph Neural Networks.(1) Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Cheekong Lee. [paper]</p> <ol> <li> <p>Abstract</p> <p>Despite the recent progress in Graph Neural Networks (GNNs), it remains challenging to explain the predictions made by GNNs. Existing explanation methods mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations for a trained GNN. The fact that post-hoc methods fail to reveal the original reasoning process of GNNs raises the need of building GNNs with built-in interpretability. In this work, we propose Prototype Graph Neural Network (ProtGNN), which combines prototype learning with GNNs and provides a new perspective on the explanations of GNNs. In ProtGNN, the explanations are naturally derived from the case-based reasoning process and are actually used during classification. The prediction of ProtGNN is obtained by comparing the inputs to a few learned prototypes in the latent space. Furthermore, for better interpretability and higher efficiency, a novel conditional subgraph sampling module is incorporated to indicate which part of the input graph is most similar to each prototype in ProtGNN+. Finally, we evaluate our method on a wide range of datasets and perform concrete case studies. Extensive results show that ProtGNN and ProtGNN+ can provide inherent interpretability while achieving accuracy on par with the non-interpretable counterparts.</p> </li> </ol>"},{"location":"topconf/AAAI/#year-2021","title":"Year 2021","text":"<p>1. Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network.(1) Seunghyun Lee, Byung Cheol Song. [paper]</p> <ol> <li> <p>Abstract</p> <p>Knowledge distillation (KD) is one of the most useful techniques for light-weight neural networks. Although neural networks have a clear purpose of embedding datasets into the low-dimensional space, the existing knowledge was quite far from this purpose and provided only limited information. We argue that good knowledge should be able to interpret the embedding procedure. This paper proposes a method of generating interpretable embedding procedure (IEP) knowledge based on principal component analysis, and distilling it based on a message passing neural network. Experimental results show that the student network trained by the proposed KD method improves 2.28% in the CIFAR100 dataset, which is a higher performance than the state-of-the-art (SOTA) method. We also demonstrate that the embedding procedure knowledge is interpretable via visualization of the proposed KD process. The implemented code is available at https://github.com/sseung0703/IEPKT.</p> </li> </ol> <p>2. Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks.(1) Yuhang Yao, Carlee Joe-Wong. [paper]</p> <ol> <li> <p>Abstract</p> <p>We study the problem of clustering nodes in a dynamic graph, where the connections between nodes and nodes' cluster memberships may change over time, e.g., due to community migration. We first propose a dynamic stochastic block model that captures these changes, and a simple decay-based clustering algorithm that clusters nodes based on weighted connections between them, where the weight decreases at a fixed rate over time. This decay rate can then be interpreted as signifying the importance of including historical connection information in the clustering. However, the optimal decay rate may differ for clusters with different rates of turnover. We characterize the optimal decay rate for each cluster and propose a clustering method that achieves almost exact recovery of the true clusters. We then demonstrate the efficacy of our clustering algorithm with optimized decay rates on simulated graph data. Recurrent neural networks (RNNs), a popular algorithm for sequence learning, use a similar decay-based method, and we use this insight to propose two new RNN-GCN (graph convolutional network) architectures for semi-supervised graph clustering. We finally demonstrate that the proposed architectures perform well on real data compared to state-of-the-art graph clustering algorithms.</p> </li> </ol>"},{"location":"topconf/CVPR/","title":"Paper Index of XAI for GNNs in CVPR","text":""},{"location":"topconf/CVPR/#year-2022","title":"Year 2022","text":"<p>1. OrphicX: A Causality-Inspired Latent Variable Model for Interpreting Graph Neural Networks.(1) Wanyu Lin, Hao Lan, Hao Wang, Baochun Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>This paper proposes a new eXplanation framework, called OrphicX, for generating causal explanations for any graph neural networks (GNNs) based on learned latent causal factors. Specifically, we construct a distinct generative model and design an objective function that encourages the generative model to produce causal, compact, and faithful explanations. This is achieved by isolating the causal factors in the latent space of graphs by maximizing the information flow measurements. We theoretically analyze the cause-effect relationships in the proposed causal graph, identify node attributes as confounders between graphs and GNN predictions, and circumvent such confounder effect by leveraging the backdoor adjustment formula. Our framework is compatible with any GNNs, and it does not require access to the process by which the target GNN produces its predictions. In addition, it does not rely on the linear-independence assumption of the explained features, nor require prior knowledge on the graph learning tasks. We show a proof-of-concept of OrphicX on canonical classification problems on graph data. In particular, we analyze the explanatory subgraphs obtained from explanations for molecular graphs (i.e., Mutag) and quantitatively evaluate the explanation performance with frequently occurring subgraph patterns. Empirically, we show that OrphicX can effectively identify the causal semantics for generating causal explanations, significantly outperforming its alternatives.</p> </li> </ol>"},{"location":"topconf/CVPR/#year-2021","title":"Year 2021","text":"<p>1. Quantifying Explainers of Graph Neural Networks in Computational Pathology.(1) Guillaume Jaume, Pushpak Pati, Behzad Bozorgtabar, Antonio Foncubierta, Anna Maria Anniciello, Florinda Feroce, Tilman Rau, Jean-Philippe Thiran, Maria Gabrani, Orcun Goksel. [paper]</p> <ol> <li> <p>Abstract</p> <p>Explainability of deep learning methods is imperative to facilitate their clinical adoption in digital pathology. However, popular deep learning methods and explainability techniques (explainers) based on pixel-wise processing disregard biological entities' notion, thus complicating comprehension by pathologists. In this work, we address this by adopting biological entity-based graph processing and graph explainers enabling explanations accessible to pathologists. In this context, a major challenge becomes to discern meaningful explainers, particularly in a standardized and quantifiable fashion. To this end, we propose herein a set of novel quantitative metrics based on statistics of class separability using pathologically measurable concepts to characterize graph explainers. We employ the proposed metrics to evaluate three types of graph explainers, namely the layer-wise relevance propagation, gradient-based saliency, and graph pruning approaches, to explain Cell-Graph representations for Breast Cancer Subtyping. The proposed metrics are also applicable in other domains by using domain-specific intuitive concepts. We validate the qualitative and quantitative findings on the BRACS dataset, a large cohort of breast cancer RoIs, by expert pathologists. The code and models will be released upon acceptance.</p> </li> </ol>"},{"location":"topconf/CVPR/#year-2019","title":"Year 2019","text":"<p>1. Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks.(1) Phillip E. Pope, Soheil Kolouri, Mohammad Rostami, Charles E. Martin, Heiko Hoffmann. [paper]</p> <ol> <li> <p>Abstract</p> <p>With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Back-Propagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.</p> </li> </ol>"},{"location":"topconf/ICDE/","title":"Paper Index of XAI for GNNs in ICDE","text":""},{"location":"topconf/ICDE/#year-2024","title":"Year 2024","text":"<p>1. SES: Bridging the Gap Between Explainability and Prediction of Graph Neural Networks.(1) Zhenhua Huang, Kunhao Li, Shaojie Wang, Zhaohong Jia, Wentao Zhu, Sharad Mehrotra. [paper]</p> <ol> <li> <p>Abstract</p> <p>Despite the Graph Neural Networks' (GNNs) pro-ficiency in analyzing graph data, achieving high-accuracy and interpretable predictions remains challenging. Existing GNN interpreters typically provide post-hoc explanations disjointed from GNNs' predictions, resulting in misrepresentations. Self-explainable GNNs offer built-in explanations during the training process. However, they cannot exploit the explanatory outcomes to augment prediction performance, and they fail to provide high-quality explanations of node features and require additional processes to generate explainable subgraphs, which is costly. To address the aforementioned limitations, we propose a self-explained and self-supervised graph neural network (SES) to bridge the gap between explainability and prediction. SES comprises two processes: explainable training and enhanced predictive learning. During explainable training, SES employs a global mask generator co-trained with a graph encoder and directly produces crucial structure and feature masks, reducing time consumption and providing node feature and subgraph explanations. In the enhanced predictive learning phase, mask-based positive-negative pairs are constructed utilizing the ex-planations to compute a triplet loss and enhance the node representations by contrastive learning. Extensive experiments demonstrate the superiority of SES on multiple datasets and tasks. SES outperforms baselines on real-world node classification datasets by notable margins of up to 2.59% and achieves state-of-the-art (SOTA) performance in explanation tasks on synthetic datasets with improvements of up to 3.0%. Moreover, SES delivers more coherent explanations on real-world datasets, has a fourfold increase in Fidelity+ score for explanation quality, and demonstrates faster training and expla-nation generating times. To our knowledge, SES is a pioneering GNN to achieve SOTA performance on both explanation and prediction tasks.</p> </li> </ol>"},{"location":"topconf/ICDE/#year-2023","title":"Year 2023","text":"<p>1. A Bayesian Graph Neural Network for EEG Classification - A Win-Win on Performance and Interpretability.(1) Jing Wang, Xiaojun Ning, Wangjun Shi, Youfang Lin. [paper]</p> <ol> <li> <p>Abstract</p> <p>With the deepening of neuroscience research, data mining of brain signals is becoming an emerging topic. Among various brain signals, electroencephalography (EEG) has attracted more and more attention due to its advantages of non-invasiveness, portability, and low cost. EEG modeling and analysis play a vital role in human healthcare. Although many machine learning algorithms have been successfully applied to data mining of EEG signals, few of them achieve a win-win in classification performance and interpretability. In this paper, we propose a Bayesian graph neural network named BayesEEGNet. Considering an electrical impulse between two nodes in the brain as a Poisson process, the countless electrical impulses generated by the brain in a period are represented as an infinite number of connection probability graphs. After coupling and transforming these probability graphs, we interpret the brain\u2019s electrical activity state as the brain\u2019s perceptual state. Benefiting from the joint optimization of Bayesian modules and deep neural networks, our model shows superior classification performance in sleep stage classification and emotion recognition tasks. Meanwhile, our model is able to learn interpretable functional connectivity relationships between EEG channels without any prior knowledge.</p> </li> </ol>"},{"location":"topconf/ICLR/","title":"Paper Index of XAI for GNNs in ICLR","text":""},{"location":"topconf/ICLR/#year-2024","title":"Year 2024","text":"<p>1. GOAt: Explaining Graph Neural Networks via Graph Output Attribution.(1) Shengyao Lu, Keith G. Mills, Jiao He, Bang Liu, Di Niu. [paper]</p> <ol> <li> <p>Abstract</p> <p>Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-of-the-art GNN explainers in terms of the commonly used fidelity metric, but also exhibits stronger discriminability, and stability by a remarkable margin.</p> </li> </ol> <p>2. GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks.(1) Peter M\u00fcller, Lukas Faber, Karolis Martinkus, Roger Wattenhofer. [paper]</p> <ol> <li> <p>Abstract</p> <p>We propose a new self-explainable Graph Neural Network (GNN) model: GraphChef. GraphChef integrates decision trees into the GNN message passing framework. Given a dataset, GraphChef returns a set of rules (a recipe) that explains each class in the dataset unlike existing GNNs and explanation methods that reason on individual graphs. Thanks to the decision trees, GraphChef recipes are human understandable. We also present a new pruning method to produce small and easy to digest trees. Experiments demonstrate that GraphChef reaches comparable accuracy to not self-explainable GNNs and produced decision trees are indeed small. We further validate the correctness of the discovered recipes on datasets where explanation ground truth is available: Reddit-Binary, MUTAG, BA-2Motifs, BA-Shapes, Tree-Cycle, and Tree-Grid.</p> </li> </ol> <p>3. GNNBoundary: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries.(1) Xiaoqi Wang, Han-Wei Shen. [paper]</p> <ol> <li> <p>Abstract</p> <p>While Graph Neural Networks (GNNs) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts to explain the decision-making process of GNNs. These efforts often focus on explaining why a certain prediction is made for a particular instance, or what discriminative features the GNNs try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of GNNs, even though the decision-making process of GNNs is directly determined by the decision boundaries. To bridge this research gap, we propose a model-level explainability method called GNNBoundary, which attempts to gain deeper insights into the decision boundaries of graph classifiers. Specifically, we first develop an algorithm to identify the pairs of classes whose decision regions are adjacent. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for boundary graph generation. Thus, by analyzing the nearboundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of GNNBoundary, we conduct experiments on both synthetic and public real-world datasets. The results demonstrate that, via the analysis of faithful near-boundary graphs generated by GNNBoundary, we can thoroughly assess the robustness and generalizability of the explained GNNs. The official implementation can be found at https://github.com/yolandalalala/GNNBoundary.</p> </li> </ol> <p>4. Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks.(1) Xu Zheng, Farhad Shirani, Tianchun Wang, Wei Cheng, Zhuomin Chen, Haifeng Chen, Hua Wei, Dongsheng Luo. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes --- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a `sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including \\(Fid_+\\), \\(Fid_-\\), and \\(Fid_\\Delta\\). Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.</p> </li> </ol>"},{"location":"topconf/ICLR/#year-2023","title":"Year 2023","text":"<p>1. DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks.(1) Wenqian Li, Yinchuan Li, Zhigang Li, Jianye Hao, Yan Pang. [paper]</p> <ol> <li> <p>Abstract</p> <p>Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure \u2013 GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its\u2019 reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.</p> </li> </ol> <p>2. GraphEx: A User-Centric Model-Level Explainer for Graph Neural Networks.(1)  Sayan Saha, Monidipa Das, Sanghamitra Bandyopadhyay. [paper]</p> <ol> <li> <p>Abstract</p> <p>With the increasing application of Graph Neural Networks (GNNs) in real-world domains, there is a growing need to understand the decision-making process of these models. To address this, we propose GraphEx, a model-level explainer that learns a graph generative model to approximate the distribution of graphs classified into a target class by the GNN model. Unlike existing methods, GraphEx does not require another black box deep model to explain the GNN and can generate a diverse set of explanation graphs with different node and edge features in one shot. Moreover, GraphEx does not need white box access to the GNN model, making it more accessible to end-users. Experiments on both synthetic and real datasets demonstrate that GraphEx can consistently produce explanations aligned with the class identity and can also identify potential limitations of the GNN model.</p> </li> </ol>"},{"location":"topconf/ICLR/#year-2021","title":"Year 2021","text":"<p>1. Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking.(1) Michael Sejr Schlichtkrull, Nicola De Cao, Ivan Titov. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph neural networks (GNNs) have become a popular approach to integrating structural inductive biases into NLP models. However, there has been little work on interpreting them, and specifically on understanding which parts of the graphs (e.g. syntactic trees or co-reference structures) contribute to a prediction. In this work, we introduce a post-hoc method for interpreting the predictions of GNNs which identifies unnecessary edges. Given a trained GNN model, we learn a simple classifier that, for every edge in every layer, predicts if that edge can be dropped. We demonstrate that such a classifier can be trained in a fully differentiable fashion, employing stochastic gates and encouraging sparsity through the expected \\(L_0\\) norm. We use our technique as an attribution method to analyze GNN models for two tasks -- question answering and semantic role labeling -- providing insights into the information flow in these models. We show that we can drop a large proportion of edges without deteriorating the performance of the model, while we can analyse the remaining edges for interpreting model predictions.</p> </li> </ol> <p>2. Image GANs meet Differentiable Rendering for Inverse Graphics and Interpretable 3D Neural Rendering.(1) Yuxuan Zhang, Wenzheng Chen, Huan Ling, Jun Gao, Yinan Zhang, Antonio Torralba, Sanja Fidler. [paper]</p> <ol> <li> <p>Abstract</p> <p>Differentiable rendering has paved the way to training neural networks to perform \u201cinverse graphics\u201d tasks such as predicting 3D geometry from monocular photographs. To train high performing models, most of the current approaches rely on multi-view imagery which are not readily available in practice.  Recent Generative Adversarial Networks (GANs) that synthesize images, in contrast, seem to acquire 3D knowledge implicitly during training: object viewpoints can be manipulated by simply manipulating the latent codes. However, these latent codes often lack further physical interpretation and thus GANs cannot easily be inverted to perform explicit 3D reasoning. In this paper, we aim to extract and disentangle 3D knowledge learned by generative models by utilizing differentiable renderers. Key to our approach is to exploit GANs as a multi-view data generator to train an inverse graphics network using an off-the-shelf differentiable renderer, and the trained inverse graphics network as a teacher to disentangle the GAN's latent code into interpretable 3D properties. The entire architecture is trained iteratively using cycle consistency losses. We show that our approach significantly outperforms state-of-the-art inverse graphics networks trained on existing datasets, both quantitatively and via user studies. We further showcase the disentangled GAN as a controllable 3D \u201cneural renderer\", complementing traditional graphics renderers.</p> </li> </ol>"},{"location":"topconf/ICML/","title":"Paper Index of XAI for GNNs in ICML","text":""},{"location":"topconf/ICML/#year-2024","title":"Year 2024","text":"<p>1. How Interpretable Are Interpretable Graph Neural Networks?(1) Yongqiang Chen, Yatao Bian, Bo Han, James Cheng. [paper]</p> <ol> <li> <p>Abstract</p> <p>Interpretable graph neural networks (XGNNs ) are widely adopted in various scientific applications involving graph-structured data. Existing XGNNs predominantly adopt the attention-based mechanism to learn edge or node importance for extracting and making predictions with the interpretable subgraph. However, the representational properties and limitations of these methods remain inadequately explored. In this work, we present a theoretical framework that formulates interpretable subgraph learning with the multilinear extension of the subgraph distribution, coined as subgraph multilinear extension (SubMT). Extracting the desired interpretable subgraph requires an accurate approximation of SubMT, yet we find that the existing XGNNs can have a huge gap in fitting SubMT. Consequently, the SubMT approximation failure will lead to the degenerated interpretability of the extracted subgraphs. To mitigate the issue, we design a new XGNN architecture called Graph Multilinear neT (GMT), which is provably more powerful in approximating SubMT. We empirically validate our theoretical findings on a number of graph classification benchmarks. The results demonstrate that GMT outperforms the state-of-the-art up to 10% in terms of both interpretability and generalizability across 12 regular and geometric graph benchmarks.</p> </li> </ol> <p>2. Explaining Graph Neural Networks via Structure-aware Interaction Index.(1) Ngoc Bui, Hieu Trung Nguyen, Viet Anh Nguyen, Rex Ying. [paper]</p> <ol> <li> <p>Abstract</p> <p>The Shapley value is a prominent tool for interpreting black-box machine learning models thanks to its strong theoretical foundation. However, for models with structured inputs, such as graph neural networks, existing Shapley-based explainability approaches either focus solely on node-wise importance or neglect the graph structure when perturbing the input instance. This paper introduces the Myerson-Taylor interaction index that internalizes the graph structure into attributing the node values and the interaction values among nodes. Unlike the Shapley-based methods, the Myerson-Taylor index decomposes coalitions into components satisfying a pre-chosen connectivity criterion. We prove that the Myerson-Taylor index is the unique one that satisfies a system of five natural axioms accounting for graph structure and high-order interaction among nodes. Leveraging these properties, we propose Myerson-Taylor Structure-Aware Graph Explainer (MAGE), a novel explainer that uses the second-order Myerson-Taylor index to identify the most important motifs influencing the model prediction, both positively and negatively. Extensive experiments on various graph datasets and models demonstrate that our method consistently provides superior subgraph explanations compared to state-of-the-art methods.</p> </li> </ol> <p>3. Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks.(1) Zhuomin Chen, Jiaxing Zhang, Jingchao Ni, Xiaoting Li, Yuchen Bian, Md Mezbahul Islam, Ananda Mondal, Hua Wei, Dongsheng Luo. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of training data but also preserve explanatory factors. Such generated proxy graphs can be reliably used to approximate the predictions of the labels of explainable subgraphs. Empirical evaluations across various datasets demonstrate our method achieves more accurate explanations for GNNs.</p> </li> </ol> <p>4. Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks.(1) Haoyu Li, Shichang Zhang, Longwen Tang, Mathieu Bauchy, Yizhou Sun. [paper]</p> <ol> <li> <p>Abstract</p> <p>Metallic Glasses (MGs) are widely used materials that are stronger than steel while being shapeable as plastic. While understanding the structure-property relationship of MGs remains a challenge in materials science, studying their energy barriers (EBs) as an intermediary step shows promise. In this work, we utilize Graph Neural Networks (GNNs) to model MGs and study EBs. We contribute a new dataset for EB prediction and a novel Symmetrized GNN (SymGNN) model that is E(3)-invariant in expectation. SymGNN handles invariance by aggregating over orthogonal transformations of the graph structure. When applied to EB prediction, SymGNN are more accurate than molecular dynamics (MD) local-sampling methods and other machine-learning models. Compared to precise MD simulations, SymGNN reduces the inference time on new MGs from roughly 41 days to less than one second. We apply explanation algorithms to reveal the relationship between structures and EBs. The structures that we identify through explanations match the medium-range order (MRO) hypothesis and possess unique topological properties. Our work enables effective prediction and interpretation of MG EBs, bolstering material science research.</p> </li> </ol>"},{"location":"topconf/ICML/#year-2023","title":"Year 2023","text":"<p>1. Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching.(1) Fang Wu, Siyuan Li, Xurui Jin, Yinghui Jiang, Dragomir Radev, Zhangming Niu, Stan Z. Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>The success of graph neural networks (GNNs) provokes the question about explainability: \u201cWhich fraction of the input graph is the most determinant of the prediction?\u201d Particularly, parametric explainers prevail in existing approaches because of their more robust capability to decipher the black-box (i.e., target GNNs). In this paper, based on the observation that graphs typically share some common motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To alleviate this issue, we design a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to fix the most informative portion of the graph and merely operates graph augmentations on the rest less informative part. Extensive experiments on synthetic and real-world datasets show the effectiveness of our MatchExplainer by outperforming all state-of-the-art parametric baselines with significant margins. Results also demonstrate that MatchDrop is a general scheme to be equipped with GNNs for enhanced performance. The code is available at https://github.com/smiles724/MatchExplainer.</p> </li> </ol> <p>2. Relevant Walk Search for Explaining Graph Neural Networks.(1)  Ping Xiong, Thomas Schnake, Michael Gastegger, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, Shinichi Nakajima. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for GNNs (GNN-LRP) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by GNN-LRP requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-\\(K\\) relevant walks, which drastically reduces the computation and thus increases the applicability of GNN-LRP to large-scale problems. Our proposed algorithms are based on the max-product algorithm\u2014a common tool for finding the maximum likelihood configurations in probabilistic graphical models\u2014and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under https://github.com/xiong-ping/rel_walk_gnnlrp.</p> </li> </ol>"},{"location":"topconf/ICML/#year-2021","title":"Year 2021","text":"<p>1. Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity.(1) Ryan Henderson, Djork-Arn\u00e9 Clevert, Floriane Montanari. [paper]</p> <ol> <li> <p>Abstract</p> <p>Rationalizing which parts of a molecule drive the predictions of a molecular graph convolutional neural network (GCNN) can be difficult. To help, we propose two simple regularization techniques to apply during the training of GCNNs: Batch Representation Orthonormalization (BRO) and Gini regularization. BRO, inspired by molecular orbital theory, encourages graph convolution operations to generate orthonormal node embeddings. Gini regularization is applied to the weights of the output layer and constrains the number of dimensions the model can use to make predictions. We show that Gini and BRO regularization can improve the accuracy of state-of-the-art GCNN attribution methods on artificial benchmark datasets. In a real-world setting, we demonstrate that medicinal chemists significantly prefer explanations extracted from regularized models. While we only study these regularizers in the context of GCNNs, both can be applied to other types of neural networks.</p> </li> </ol> <p>2. On Explainability of Graph Neural Networks via Subgraph Explorations.(1) Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, Shuiwang Ji. [paper]</p> <ol> <li> <p>Abstract</p> <p>We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.</p> </li> </ol>"},{"location":"topconf/IJCAI/","title":"Paper Index of XAI for GNNs in IJCAI","text":""},{"location":"topconf/IJCAI/#year-2023","title":"Year 2023","text":"<p>1. Interpret ESG Rating's Impact on the Industrial Chain Using Graph Neural Networks.(1) Bin Liu, Jiujun He, Ziyuan Li, Xiaoyang Huang, Xiang Zhang, Guosheng Yin. [paper]</p> <ol> <li> <p>Abstract</p> <p>We conduct a quantitative analysis of the development of the industry chain from the environmental, social, and governance (ESG) perspective, which is an overall measure of sustainability. Factors that may impact the performance of the industrial chain have been studied in the literature, such as government regulation, monetary policy, etc. Our interest lies in how the sustainability change (i.e., ESG shock) affects the performance of the industrial chain. To achieve this goal, we model the industrial chain with a graph neural network (GNN) and conduct node regression on two financial performance metrics, namely, the aggregated profitability ratios and operating margin. To quantify the effects of ESG, we propose to compute the interaction between ESG shocks and industrial chain features with a cross-attention module, and then filter the original node features in the graph regression. Experiments on two real datasets demonstrate that (i) there are significant effects of ESG shocks on the industrial chain, and (ii) model parameters including regression coefficients and the attention map can explain how ESG shocks affect the performance of the industrial chain.</p> </li> </ol>"},{"location":"topconf/IJCAI/#year-2021","title":"Year 2021","text":"<p>1. Smart Contract Vulnerability Detection: From Pure Neural Network to Interpretable Graph Feature and Expert Pattern Fusion.(1) Zhenguang Liu, Peng Qian, Xiang Wang, Lei Zhu, Qinming He, Shouling Ji. [paper]</p> <ol> <li> <p>Abstract</p> <p>Smart contracts hold digital coins worth billions of dollars, their security issues have drawn extensive attention in the past years. Towards smart contract vulnerability detection, conventional methods heavily rely on fixed expert rules, leading to low accuracy and poor scalability. Recent deep learning approaches alleviate this issue but fail to encode useful expert knowledge. In this paper, we explore combining deep learning with expert patterns in an explainable fashion. Specifically, we develop automatic tools to extract expert patterns from the source code. We then cast the code into a semantic graph to extract deep graph features. Thereafter, the global graph feature and local expert patterns are fused to cooperate and approach the final prediction, while yielding their interpretable weights. Experiments are conducted on all available smart contracts with source code in two platforms, Ethereum and VNT Chain. Empirically, our system significantly outperforms state-of-the-art methods. Our code is released.</p> </li> </ol>"},{"location":"topconf/KDD/","title":"Paper Index of XAI for GNNs in KDD","text":""},{"location":"topconf/KDD/#year-2024","title":"Year 2024","text":"<p>1. Unveiling Global Interactive Patterns across Graphs: Towards Interpretable Graph Neural Networks.(1) Yuwen Wang, Shunyu Liu, Tongya Zheng, Kaixuan Chen, Mingli Song. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have emerged as a prominent framework for graph mining, leading to significant advances across various domains. Stemmed from the node-wise representations of GNNs, existing explanation studies have embraced the subgraph-specific viewpoint that attributes the decision results to the salient features and local structures of nodes. However, graph-level tasks necessitate long-range dependencies and global interactions for advanced GNNs, deviating significantly from subgraph-specific explanations. To bridge this gap, this paper proposes a novel intrinsically interpretable scheme for graph classification, termed as Global Interactive Pattern (GIP) learning, which introduces learnable global interactive patterns to explicitly interpret decisions. GIP first tackles the complexity of interpretation by clustering numerous nodes using a constrained graph clustering module. Then, it matches the coarsened global interactive instance with a batch of self-interpretable graph prototypes, thereby facilitating a transparent graph-level reasoning process. Extensive experiments conducted on both synthetic and real-world benchmarks demonstrate that the proposed GIP yields significantly superior interpretability and competitive performance to the state-of-the-art counterparts. Our code will be made publicly available.</p> </li> </ol>"},{"location":"topconf/KDD/#year-2023","title":"Year 2023","text":"<p>1. Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks.(1) Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, Yujun Yan. [paper]</p> <ol> <li> <p>Abstract</p> <p>Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications that can be formulated as graph classification tasks. However, dense brain graphs pose computational challenges such as large time and memory consumption and poor model interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. Many prior works select noisy edges based on explainability or task-irrelevant properties, but this does not guarantee performance improvement when using the sparsified graphs. Additionally, the selection of noisy edges is often tailored to each individual graph, making it challenging to sparsify multiple graphs collectively using the same approach.</p> <p>To address the issues above, we first introduce an iterative framework to analyze the effectiveness of different sparsification models. By utilizing this framework, we find that (i) methods that prioritize interpretability may not be suitable for graph sparsification, as the sparsified graphs may degenerate the performance of GNN models; (ii) it is beneficial to learn the edge selection during the training of the GNN, rather than after the GNN has converged; (iii) learning a joint edge selection shared across all graphs achieves higher performance than generating separate edge selection for each graph; and (iv) gradient information, which is task-relevant, helps with edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which improves the graph classification performance by up to 5.1% with 55.0% fewer edges than the original graphs. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature.</p> </li> </ol>"},{"location":"topconf/NeurIPS/","title":"Paper Index of XAI for GNNs in NeurIPS","text":""},{"location":"topconf/NeurIPS/#year-2023","title":"Year 2023","text":"<p>1. TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery.(1) Jialin Chen, Rex Ying. [paper]</p> <ol> <li> <p>Abstract</p> <p>Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.</p> </li> </ol> <p>2. V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs.(1) Senzhang Wang, Jun Yin, Chaozhuo Li, Xing Xie, Jianxin Wang. [paper]</p> <ol> <li> <p>Abstract</p> <p>GNN explanation method aims to identify an explanatory subgraph which contains the most informative components of the full graph. However, a major limitation of existing GNN explainers is that they are not robust to the structurally corrupted graphs, e.g., graphs with noisy or adversarial edges. On the one hand, existing GNN explainers mostly explore explanations based on either the raw graph features or the learned latent representations, both of which can be easily corrupted. On the other hand, the corruptions in graphs are irregular in terms of the structural properties, e.g., the size or connectivity of graphs, which makes the rigorous constraints used by previous GNN explainers unfeasible. To address these issues, we propose a robust GNN explainer called V-InfoR. Specifically, a robust graph representation extractor, which takes insights of variational inference, is proposed to infer the latent distribution of graph representations. Instead of directly using the corrupted raw features or representations of each single graph, we sample the graph representations from the inferred distribution for the downstream explanation generator, which can effectively eliminate the minor corruption. We next formulate the explanation exploration as a graph information bottleneck (GIB) optimization problem. As a more general method that does not need any rigorous structural constraints, our GIB-based method can adaptively capture both the regularity and irregularity of the severely corrupted graphs for explanation. Extensive evaluations on both synthetic and real-world datasets indicate that V-InfoR significantly improves the GNN explanation performance for the structurally corrupted graphs. Code and dataset are available at https://anonymous.4open.science/r/V-InfoR-EF88.</p> </li> </ol> <p>3. Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks.(1) Jun Yin, Chaozhuo Li, Hao Yan, Jianxun Lian, Senzhang Wang. [paper]</p> <ol> <li> <p>Abstract</p> <p>Intrinsic interpretable graph neural networks aim to provide transparent predictions by identifying the influential fraction of the input graph that guides the model prediction, i.e., the explanatory subgraph. However, current interpretable GNNs mostly are dataset-specific and hard to generalize to different graphs. A more generalizable GNN interpretation model which can effectively distill the universal structural patterns of different graphs is until-now unexplored. Motivated by the great success of recent pre-training techniques, we for the first time propose the Pre-training Interpretable Graph Neural Network (\\(\\pi\\)-GNN) to distill the universal interpretability of GNNs by pre-training over synthetic graphs with ground-truth explanations. Specifically, we introduce a structural pattern learning module to extract diverse universal structure patterns and integrate them together to comprehensively represent the graphs of different types. Next, a hypergraph refining module is proposed to identify the explanatory subgraph by incorporating the universal structure patterns with local edge interactions. Finally, the task-specific predictor is cascaded with the pre-trained \\(\\pi\\)-GNN model and fine-tuned over downstream tasks. Extensive experiments demonstrate that \\(\\pi\\)-GNN significantly surpasses the leading interpretable GNN baselines with up to 9.98\\% interpretation improvement and 16.06\\% classification accuracy improvement. Meanwhile, \\(\\pi\\)-GNN pre-trained on graph classification task also achieves the top-tier interpretation performance on node classification task, which further verifies its promising generalization performance among different downstream tasks. Our code and datasets are available at https://anonymous.4open.science/r/PI-GNN-F86C.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2022","title":"Year 2022","text":"<p>1. GStarX: Explaining Graph Neural Networks with Structure-Aware Cooperative Games.(1) Shichang Zhang, Yozen Liu, Neil Shah, Yizhou Sun. [paper]</p> <ol> <li> <p>Abstract</p> <p>Explaining machine learning models is an important and increasingly popular area of research interest. The Shapley value from game theory has been proposed as a prime approach to compute feature importance towards model predictions on images, text, tabular data, and recently graph neural networks (GNNs) on graphs. In this work, we revisit the appropriateness of the Shapley value for GNN explanation, where the task is to identify the most important subgraph and constituent nodes for GNN predictions. We claim that the Shapley value is a non-ideal choice for graph data because it is by definition not structure-aware. We propose a Graph Structure-aware eXplanation (GStarX) method to leverage the critical graph structure information to improve the explanation. Specifically, we define a scoring function based on a new structure-aware value from the cooperative game theory proposed by Hamiache and Navarro (HN). When used to score node importance, the HN value utilizes graph structures to attribute cooperation surplus between neighbor nodes, resembling message passing in GNNs, so that node importance scores reflect not only the node feature importance, but also the node structural roles. We demonstrate that GStarX produces qualitatively more intuitive explanations, and quantitatively improves explanation fidelity over strong baselines on chemical graph property prediction and text graph sentiment classification. Code: https://github.com/ShichangZh/GStarX.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2021","title":"Year 2021","text":"<p>1. Learning Theory Can (Sometimes) Explain Generalisation in Graph Neural Networks.(1) Pascal Esser, Leena Chennuru Vankadara, Debarghya Ghoshdastidar. [paper]</p> <ol> <li> <p>Abstract</p> <p>In recent years, several results in the supervised learning setting suggested that classical statistical learning-theoretic measures, such as VC dimension, do not adequately explain the performance of deep learning models which prompted a slew of work in the infinite-width and iteration regimes. However, there is little theoretical explanation for the success of neural networks beyond the supervised setting. In this paper we argue that, under some distributional assumptions, classical learning-theoretic measures can sufficiently explain generalization for graph neural networks in the transductive setting. In particular, we provide a rigorous analysis of the performance of neural networks in the context of transductive inference, specifically by analysing the generalisation properties of graph convolutional networks for the problem of node classification. While VC-dimension does result in trivial generalisation error bounds in this setting as well, we show that transductive Rademacher complexity can explain the generalisation properties of graph convolutional networks for stochastic block models. We further use the generalisation error bounds based on transductive Rademacher complexity to demonstrate the role of graph convolutions and network architectures in achieving smaller generalisation error and provide insights into when the graph structure can help in learning. The findings of this paper could re-new the interest in studying generalisation in neural networks in terms of learning-theoretic measures, albeit in specific problems.</p> </li> </ol> <p>2. Reinforcement Learning Enhanced Explainer for Graph Neural Networks.(1)  Caihua Shan, Yifei Shen, Yao Zhang, Xiang Li, Dongsheng Li. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph neural networks (GNNs) have recently emerged as revolutionary technologies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most influential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization problem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a connected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme. Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability.</p> </li> </ol> <p>3. Towards Multi-Grained Explainability for Graph Neural Networks.(1) Xiang Wang, Yingxin Wu, An Zhang, Xiangnan He, Tat-Seng Chua2. [paper]</p> <ol> <li> <p>Abstract</p> <p>When a graph neural network (GNN) made a prediction, one raises question about explainability: \u201cWhich fraction of the input graph is most in\ufb02uential to the model\u2019s decision?\u201d Producing an answer requires understanding the model\u2019s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the \ufb02exibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and \ufb01ne-tuning idea to develop our explainer and generate multi-grained explanations. Speci\ufb01cally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the \ufb01ne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classi\ufb01cation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.</p> </li> </ol>"},{"location":"topconf/NeurIPS/#year-2020","title":"Year 2020","text":"<p>1. Parameterized Explainer for Graph Neural Network.(1) Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, Xiang Zhang. [paper]</p> <ol> <li> <p>Abstract</p> <p>Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging open problem. The leading method mainly addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufficient to provide a global understanding of the learned GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to multi-instance explanations. Compared to the existing work, PGExplainer has a better generalization power and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7\\% relative improvement in AUC on explaining graph classification over the leading baseline.</p> </li> </ol> <p>2. PGM-Explainer: Probabilistic Graphical Model Explanations for Graph Neural Networks.(1) Minh Vu, My T. Thai. [paper]</p> <ol> <li> <p>Abstract</p> <p>In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.</p> </li> </ol>"},{"location":"topconf/WWW/","title":"Paper Index of XAI for GNNs in WWW","text":""},{"location":"topconf/WWW/#year-2024","title":"Year 2024","text":"<p>1. Adversarial Mask Explainer for Graph Neural Networks.(1) Wei Zhang, Xiaofan Li, Wolfgang Nejdl. [paper]</p> <ol> <li> <p>Abstract</p> <p>The Graph Neural Networks (GNNs) model is a powerful tool for integrating node information with graph topology to learn representations and make predictions. However, the complex graph structure of GNNs has led to a lack of clear explainability in the decision-making process. Recently, there has been a growing interest in seeking instance-level explanations of the GNNs model, which aims to uncover the decision-making process of the GNNs model and provide insights into how it arrives at its final output. Previous works have focused on finding a set of weights (masks) for edges/nodes/node features to determine their importance. These works have adopted a regularization term and a hyperparameter K to control the explanation size during the training process and keep only the top-K weights as the explanation set. However, the true size of the explanation is typically unknown to users, making it difficult to provide reasonable values for the regularization term and K. In this work, we propose a novel framework AMExplainer which leverages the concept of adversarial networks to achieve a dual optimization objective in the target function. This approach ensures both accurate prediction of the mask and sparsity of the explanation set. In addition, we devise a novel scaling function to automatically sense and amplify the weights of the informative part of the graph, which filters out insignificant edges/nodes/node features for expediting the convergence of the solution during training. Our extensive experiments show that AMExplainer yields a more compelling explanation by generating a sparse set of masks while simultaneously maintaining fidelity.</p> </li> </ol>"},{"location":"topconf/WWW/#year-2021","title":"Year 2021","text":"<p>1. Interpreting and Unifying Graph Neural Networks with An Optimization Framework.(1) Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, Peng Cui:. [paper]</p> <ol> <li> <p>Abstract</p> <p>Graph Neural Networks (GNNs) have received considerable attention on graph-structured data learning for a wide variety of tasks. The well-designed propagation mechanism which has been demonstrated effective is the most fundamental part of GNNs. Although most of GNNs basically follow a message passing manner, litter effort has been made to discover and analyze their essential relations. In this paper, we establish a surprising connection between different propagation mechanisms with a unified optimization problem, showing that despite the proliferation of various GNNs, in fact, their proposed propagation mechanisms are the optimal solution optimizing a feature fitting function over a wide class of graph kernels with a graph regularization term. Our proposed unified optimization framework, summarizing the commonalities between several of the most representative GNNs, not only provides a macroscopic view on surveying the relations between different GNNs, but also further opens up new opportunities for flexibly designing new GNNs. With the proposed framework, we discover that existing works usually utilize na\u00efve graph convolutional kernels for feature fitting function, and we further develop two novel objective functions considering adjustable graph kernels showing low-pass or high-pass filtering capabilities respectively. Moreover, we provide the convergence proofs and expressive power comparisons for the proposed models. Extensive experiments on benchmark datasets clearly show that the proposed GNNs not only outperform the state-of-the-art methods but also have good ability to alleviate over-smoothing, and further verify the feasibility for designing GNNs with our unified optimization framework.</p> </li> </ol>"}]}